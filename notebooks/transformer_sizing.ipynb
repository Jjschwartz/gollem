{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Sizing\n",
    "\n",
    "This notebook runs a bunch of analysis about a GPT-2 Transformer, e.g. number of FLOPS, parameters, peak memory footprint, checkpoint size, etc\n",
    "\n",
    "**Reference**\n",
    "- This notebook is based directly on Karpathy's [nanoGPT/transformer_sizing.ipynb](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gollem.models.gpt2.config import get_gpt2_model_config\n",
    "from gollem.models.llama3.config import get_llama3_model_config\n",
    "from gollem.gpu_stats import get_gpu_flops_for_all_gpus\n",
    "from gollem.gpu_stats import get_gpu_flops\n",
    "from gollem.gpu_stats import GPU_INFO\n",
    "\n",
    "# NOTE: change things here for your model\n",
    "# ----------------------------------------\n",
    "# Change this to the model you want to analyze\n",
    "model_name = \"gpt2\"\n",
    "# observed checkpoint size\n",
    "measured_checkpoint_size_bytes: int | None = None\n",
    "# measured throughput\n",
    "measured_tokens_per_second = 27000\n",
    "# what GPU we used?\n",
    "gpu_name = \"H100\"\n",
    "# what precision we used? (float32, float16, bfloat16, int8, etc)\n",
    "dtype = \"float32\"\n",
    "# ----------------------------------------\n",
    "\n",
    "if model_name.startswith(\"gpt\"):\n",
    "    model_cfg = get_gpt2_model_config(model_name)\n",
    "elif model_name.startswith(\"llama\"):\n",
    "    model_cfg = get_llama3_model_config(model_name)\n",
    "else:\n",
    "    raise ValueError(f\"Model name {model_name} not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count\n",
    "\n",
    "Here we look at the total number of parameters and the breakdown by component for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = model_cfg.get_params()\n",
    "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "for k, v in model_params.per_component.items():\n",
    "    print(f\"{k:20s} {v:10d} {v / model_params.total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter/Checkpoint Size\n",
    "\n",
    "We can now calculate the size of each checkpoint.\n",
    "\n",
    "Noting that params are stored in fp32 (i.e. 4 bytes), and the AdamW optimizer has 2 additional buffers per param for statistics.\n",
    "\n",
    "We can also compare this to the observed checkpoint size, if we have it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_bytes = model_params.total * 4\n",
    "params_and_buffers_bytes = params_bytes + 2 * params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes / 1e9:.2f} GB\")\n",
    "\n",
    "if measured_checkpoint_size_bytes is not None:\n",
    "    print(f\"measured with wc -c ckpt.pt: {measured_checkpoint_size_bytes}\")\n",
    "    print(\n",
    "        f\"fluff ratio: {measured_checkpoint_size_bytes / params_and_buffers_bytes * 100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU memory usage\n",
    "\n",
    "We can estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer for different GPU sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nvidia reports memory in GiB (denoted as GB but is actually GiB)\n",
    "print(\"GPU memory ratio taken up just for parameters (incl. optimizer)\")\n",
    "print(f\"{'GPU':12s} {'ratio (%)':8s}\")\n",
    "for k, v in GPU_INFO.items():\n",
    "    print(f\"{k:12s} {params_and_buffers_bytes / v.memory_bytes * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU forward-backward memory usage\n",
    "\n",
    "We can estimate the total memory usage of the model over the course of a forward-backward pass.\n",
    "\n",
    "This is made up of:\n",
    "\n",
    "1. M_model - the model and optimizer parameters\n",
    "2. M_optimizer - the optimizer buffers\n",
    "3. M_gradient - the gradient of the model\n",
    "4. M_activations - the activations of the model\n",
    "\n",
    "The activations scale with batch size and are typically the largest component of memory usage, since the others are fixed given model size and context length.\n",
    "\n",
    "Calculating the activation memory usage is quite tricky since it is affected by low level optimizations that can be hard to estimate exactly. It is also affected by things like flash attention, dropout, activation checkpointing, etc.\n",
    "\n",
    "Another source of memory usage is the CUDA and pytorch overhead which is typically 0.5-2GiB and will depend on the setup of the system that is running. We don't include this in the calculations, so factor this in when comparing calculated vs empirical memory usage.\n",
    "\n",
    "NOTE: these are estimates, so really should be taken as ballpark figures rather than exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when using bf16 or fp16 we use mixed precision so have tostore\n",
    "# both fp16 and fp32 versions of the parameters\n",
    "# when using fp32 we store only the full precision fp32 parameters\n",
    "bytes_per_param = 6 if dtype in (\"bfloat16\", \"float16\") else 4\n",
    "M_model = model_params.total * bytes_per_param\n",
    "\n",
    "# AdamW optimizer has 2 buffers per parameter\n",
    "# values are stored in fp32\n",
    "M_optimizer = 2 * model_params.total * 4\n",
    "\n",
    "# we store one gradient value per parameter\n",
    "# Gradient are stored in fp32\n",
    "M_gradient = model_params.total * 4\n",
    "\n",
    "divisor = 1024**3\n",
    "\n",
    "print(f\"M_model: {M_model / divisor:.2f} GiB\")\n",
    "print(f\"M_optimizer: {M_optimizer / divisor:.2f} GiB\")\n",
    "print(f\"M_gradient: {M_gradient / divisor:.2f} GiB\")\n",
    "\n",
    "# M_activations is more complex\n",
    "print(f\"{'batch size':10s} {'M_activations':13s} {'total memory':12s}\")\n",
    "for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "    M_activations = model_cfg.compute_activations(batch_size, dtype)\n",
    "    total_memory = M_model + M_optimizer + M_activations.total + M_gradient\n",
    "    total_memory_GiB = total_memory / divisor\n",
    "    print(\n",
    "        f\"{batch_size:10d} {M_activations.total / divisor:13.2f} {total_memory_GiB:12.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOPS\n",
    "\n",
    "Now let's looks at the FLOPS used by the model and compare to the estimate from the PaLM paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flops = model_cfg.compute_flops()\n",
    "flops_total = model_flops.forward_total\n",
    "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
    "for k, v in model_flops.per_component.items():\n",
    "    print(f\"{k:20s} {v:14d} {v / flops_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFU\n",
    "\n",
    "Given our estimated FLOPS we can calculate how much of our GPU's FLOP capacity is being used, that is our model flop utilization (MFU).\n",
    "\n",
    "To calculate this we need 3 bits of information:\n",
    "\n",
    "- GPU speed (FLOPS)\n",
    "- observer throughput (tokens per second)\n",
    "- model flops (FLOPs) per iteration\n",
    "\n",
    "For the GPU speed we refer to: https://www.techpowerup.com/gpu-specs/ which gives theoretical performance, specifically looking at performance for BF16 and FP16, which ever is supported and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate flops achieved\n",
    "# Get model flops per token (note we need to divide by context length\n",
    "# to get the flops per token)\n",
    "model_flops_per_token = model_cfg.compute_flops().total / model_cfg.n_ctx\n",
    "gpu_flops = get_gpu_flops(gpu_name, dtype)\n",
    "\n",
    "print(f\"{'GPU':10s} {'MFU (%)':8s}\")\n",
    "mfu = (model_flops_per_token * measured_tokens_per_second) / gpu_flops\n",
    "print(f\"{gpu_name:10s} {mfu * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Training Compute\n",
    "\n",
    "Here we use the value computed so far to compute an estimate of the total amount of compute needed to train the model.\n",
    "\n",
    "Here we estimate the total amount of compute `C` required to train the model as `C ~= 6*N*D`, where:\n",
    "\n",
    "- `6` is a heuristic value (see [Dzmitry Bahdanau's post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) for an explanation) \n",
    "    - it basically stems from weight multiplications requiring 6 FLOPS per token per weight when combining both forward and backward passes.\n",
    "- `N` is the total number of model parameters\n",
    "- `D` is total size of dataset (in tokens)\n",
    "\n",
    "Note the equation changes to `C ~= 8*N*D` with activation checkpointing (i.e. recompute activations as needed to save memory when doing back-prop).\n",
    "\n",
    "We also need to factor in the model flops utilization (MFU) to correct for the fact that we cannot use 100% of the GPUs FLOPS due to memory bottlenecks, etc (again see Dzmitry's blog for examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "model_size = model_cfg.get_params().total  # this is number of parameters, N\n",
    "\n",
    "# TODO change these parameters\n",
    "tokens_num = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "num_gpus = 8  # number of GPUS used in parallel\n",
    "\n",
    "print(\"Time needed to train model on different GPUS\")\n",
    "print(f\"{'GPU':10s} {'time (days)':16s}\")\n",
    "for gpu_name, gpu_flops in get_gpu_flops_for_all_gpus(dtype).items():\n",
    "    flops_throughput = gpu_flops * num_gpus * assumed_mfu\n",
    "    flops_needed = 6 * model_size * tokens_num  # 6ND\n",
    "    time_needed_s = flops_needed / flops_throughput  # in seconds\n",
    "    print(f\"{gpu_name:10s} {time_needed_s / 3600 / 24:10.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
