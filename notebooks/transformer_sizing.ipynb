{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Sizing\n",
    "\n",
    "This notebook runs a bunch of analysis about a GPT-2 Transformer, e.g. number of FLOPS, parameters, peak memory footprint, checkpoint size, etc\n",
    "\n",
    "**Reference**\n",
    "- This notebook is based directly on Karpathy's [nanoGPT/transformer_sizing.ipynb](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gollem.models.gpt2.config import get_gpt2_model_config\n",
    "from gollem.models.llama3.config import get_llama3_model_config\n",
    "from gollem.gpu_stats import get_gpu_flops_for_all_gpus\n",
    "from gollem.gpu_stats import get_gpu_flops\n",
    "from gollem.gpu_stats import GPU_INFO\n",
    "\n",
    "# NOTE: change things here for your model\n",
    "# ----------------------------------------\n",
    "# Change this to the model you want to analyze\n",
    "model_name = \"gpt2-xl\"\n",
    "# observed checkpoint size\n",
    "measured_checkpoint_size_bytes: int | None = None\n",
    "# measured throughput\n",
    "measured_tokens_per_second = 40377\n",
    "# what GPU we used?\n",
    "gpu_name = \"H100\"\n",
    "# what precision we used? (float32, float16, bfloat16, int8, etc)\n",
    "dtype = \"bfloat16\"\n",
    "# ----------------------------------------\n",
    "\n",
    "if model_name.startswith(\"gpt\"):\n",
    "    model_cfg = get_gpt2_model_config(model_name)\n",
    "elif model_name.startswith(\"llama\"):\n",
    "    model_cfg = get_llama3_model_config(model_name)\n",
    "else:\n",
    "    raise ValueError(f\"Model name {model_name} not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count\n",
    "\n",
    "Here we look at the total number of parameters and the breakdown by component for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 params     ratio (%) \n",
      "embedding/position      1638400     0.1052\n",
      "embedding/token        80411200     5.1635\n",
      "embedding              82049600     5.2687\n",
      "attention/ln               3200     0.0002\n",
      "attention/kqv           7680000     0.4932\n",
      "attention/proj          2560000     0.1644\n",
      "attention              10243200     0.6578\n",
      "mlp/ln                     3200     0.0002\n",
      "mlp/ffw                10246400     0.6580\n",
      "mlp/proj               10241600     0.6576\n",
      "mlp                    20491200     1.3158\n",
      "block                  30734400     1.9736\n",
      "transformer          1475251200    94.7311\n",
      "ln_f                       3200     0.0002\n",
      "out_embedding                 0     0.0000\n",
      "total                1557304000   100.0000\n"
     ]
    }
   ],
   "source": [
    "model_params = model_cfg.get_params()\n",
    "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "for k, v in model_params.per_component.items():\n",
    "    print(f\"{k:20s} {v:10d} {v / model_params.total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter/Checkpoint Size\n",
    "\n",
    "We can now calculate the size of each checkpoint.\n",
    "\n",
    "Noting that params are stored in fp32 (i.e. 4 bytes), and the AdamW optimizer has 2 additional buffers per param for statistics.\n",
    "\n",
    "We can also compare this to the observed checkpoint size, if we have it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est checkpoint size: 18.69 GB\n"
     ]
    }
   ],
   "source": [
    "params_bytes = model_params.total * 4\n",
    "params_and_buffers_bytes = params_bytes + 2 * params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes / 1e9:.2f} GB\")\n",
    "\n",
    "if measured_checkpoint_size_bytes is not None:\n",
    "    print(f\"measured with wc -c ckpt.pt: {measured_checkpoint_size_bytes}\")\n",
    "    print(\n",
    "        f\"fluff ratio: {measured_checkpoint_size_bytes / params_and_buffers_bytes * 100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU memory usage\n",
    "\n",
    "We can estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer for different GPU sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory ratio taken up just for parameters (incl. optimizer)\n",
      "GPU          ratio (%)\n",
      "H100            21.76\n",
      "A100            21.76\n",
      "RTX4090         72.52\n",
      "RTX3090         72.52\n"
     ]
    }
   ],
   "source": [
    "# Nvidia reports memory in GiB (denoted as GB but is actually GiB)\n",
    "print(\"GPU memory ratio taken up just for parameters (incl. optimizer)\")\n",
    "print(f\"{'GPU':12s} {'ratio (%)':8s}\")\n",
    "for k, v in GPU_INFO.items():\n",
    "    print(f\"{k:12s} {params_and_buffers_bytes / v.memory_bytes * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU forward-backward memory usage\n",
    "\n",
    "We can estimate the total memory usage of the model over the course of a forward-backward pass.\n",
    "\n",
    "This is made up of:\n",
    "\n",
    "1. M_model - the model and optimizer parameters\n",
    "2. M_optimizer - the optimizer buffers\n",
    "3. M_gradient - the gradient of the model\n",
    "4. M_activations - the activations of the model\n",
    "\n",
    "The activations scale with batch size and are typically the largest component of memory usage, since the others are fixed given model size and context length.\n",
    "\n",
    "Calculating the activation memory usage is quite tricky since it is affected by low level optimizations that can be hard to estimate exactly. It is also affected by things like flash attention, dropout, activation checkpointing, etc.\n",
    "\n",
    "Another source of memory usage is the CUDA and pytorch overhead which is typically 0.5-2GiB and will depend on the setup of the system that is running. We don't include this in the calculations, so factor this in when comparing calculated vs empirical memory usage.\n",
    "\n",
    "NOTE: these are estimates, so really should be taken as ballpark figures rather than exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_model: 8.70 GiB\n",
      "M_optimizer: 11.60 GiB\n",
      "M_gradient: 5.80 GiB\n",
      "batch size M_activations total memory\n",
      "         1          2.35        28.46\n",
      "         2          4.70        30.81\n",
      "         4          9.40        35.51\n",
      "         8         18.80        44.91\n",
      "        16         37.60        63.71\n",
      "        32         75.20       101.31\n"
     ]
    }
   ],
   "source": [
    "# when using bf16 or fp16 we use mixed precision so have tostore\n",
    "# both fp16 and fp32 versions of the parameters\n",
    "# when using fp32 we store only the full precision fp32 parameters\n",
    "bytes_per_param = 6 if dtype in (\"bfloat16\", \"float16\") else 4\n",
    "M_model = model_params.total * bytes_per_param\n",
    "\n",
    "# AdamW optimizer has 2 buffers per parameter\n",
    "# values are stored in fp32\n",
    "M_optimizer = 2 * model_params.total * 4\n",
    "\n",
    "# we store one gradient value per parameter\n",
    "# Gradient are stored in fp32\n",
    "M_gradient = model_params.total * 4\n",
    "\n",
    "divisor = 1024**3\n",
    "\n",
    "print(f\"M_model: {M_model / divisor:.2f} GiB\")\n",
    "print(f\"M_optimizer: {M_optimizer / divisor:.2f} GiB\")\n",
    "print(f\"M_gradient: {M_gradient / divisor:.2f} GiB\")\n",
    "\n",
    "# M_activations is more complex\n",
    "print(f\"{'batch size':10s} {'M_activations':13s} {'total memory':12s}\")\n",
    "for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "    M_activations = model_cfg.compute_activations(batch_size, dtype)\n",
    "    total_memory = M_model + M_optimizer + M_activations.total + M_gradient\n",
    "    total_memory_GiB = total_memory / divisor\n",
    "    print(\n",
    "        f\"{batch_size:10d} {M_activations.total / divisor:13.2f} {total_memory_GiB:12.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOPS\n",
    "\n",
    "Now let's looks at the FLOPS used by the model and compare to the estimate from the PaLM paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 flops          ratio (%) \n",
      "attention/kqv           15728640000     0.4485\n",
      "attention/scores         3355443200     0.0957\n",
      "attention/reduce         3355443200     0.0957\n",
      "attention/proj           5242880000     0.1495\n",
      "attention               27682406400     0.7894\n",
      "mlp/ffw1                20971520000     0.5980\n",
      "mlp/ffw2                20971520000     0.5980\n",
      "mlp                     41943040000     1.1961\n",
      "block                   69625446400     1.9855\n",
      "transformer           3342021427200    95.3038\n",
      "out_embedding          164682137600     4.6962\n",
      "forward_total         3506703564800   100.0000\n",
      "backward_total        7013407129600   200.0000\n",
      "total                10520110694400   300.0000\n"
     ]
    }
   ],
   "source": [
    "model_flops = model_cfg.compute_flops()\n",
    "flops_total = model_flops.forward_total\n",
    "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
    "for k, v in model_flops.per_component.items():\n",
    "    print(f\"{k:20s} {v:14d} {v / flops_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFU\n",
    "\n",
    "Given our estimated FLOPS we can calculate how much of our GPU's FLOP capacity is being used, that is our model flop utilization (MFU).\n",
    "\n",
    "To calculate this we need 3 bits of information:\n",
    "\n",
    "- GPU speed (FLOPS)\n",
    "- observer throughput (tokens per second)\n",
    "- model flops (FLOPs) per iteration\n",
    "\n",
    "For the GPU speed we refer to: https://www.techpowerup.com/gpu-specs/ which gives theoretical performance, specifically looking at performance for BF16 and FP16, which ever is supported and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_flops_per_token 10273545600.0\n",
      "gpu_flops: 9.89e+14\n",
      "GPU        MFU (%) \n",
      "H100          41.94\n"
     ]
    }
   ],
   "source": [
    "# calculate flops achieved\n",
    "# Get model flops per token (note we need to divide by context length\n",
    "# to get the flops per token)\n",
    "model_flops_per_token = model_cfg.compute_flops().total / model_cfg.n_ctx\n",
    "gpu_flops = get_gpu_flops(gpu_name, dtype)\n",
    "\n",
    "print(\"model_flops_per_token\", model_flops_per_token)\n",
    "print(f\"gpu_flops: {gpu_flops:.2e}\")\n",
    "\n",
    "print(f\"{'GPU':10s} {'MFU (%)':8s}\")\n",
    "mfu = (model_flops_per_token * measured_tokens_per_second) / gpu_flops\n",
    "print(f\"{gpu_name:10s} {mfu * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Training Compute\n",
    "\n",
    "Here we use the value computed so far to compute an estimate of the total amount of compute needed to train the model.\n",
    "\n",
    "Here we estimate the total amount of compute `C` required to train the model as `C ~= 6*N*D`, where:\n",
    "\n",
    "- `6` is a heuristic value (see [Dzmitry Bahdanau's post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) for an explanation) \n",
    "    - it basically stems from weight multiplications requiring 6 FLOPS per token per weight when combining both forward and backward passes.\n",
    "- `N` is the total number of model parameters\n",
    "- `D` is total size of dataset (in tokens)\n",
    "\n",
    "Note the equation changes to `C ~= 8*N*D` with activation checkpointing (i.e. recompute activations as needed to save memory when doing back-prop).\n",
    "\n",
    "We also need to factor in the model flops utilization (MFU) to correct for the fact that we cannot use 100% of the GPUs FLOPS due to memory bottlenecks, etc (again see Dzmitry's blog for examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to train model on different GPUS\n",
      "GPU        time (days)     \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "BF16 FLOPS not available for RTX4090",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime needed to train model on different GPUS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime (days)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m16s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gpu_name, gpu_flops \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_gpu_flops_for_all_gpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     12\u001b[0m     flops_throughput \u001b[38;5;241m=\u001b[39m gpu_flops \u001b[38;5;241m*\u001b[39m num_gpus \u001b[38;5;241m*\u001b[39m assumed_mfu\n\u001b[1;32m     13\u001b[0m     flops_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m model_size \u001b[38;5;241m*\u001b[39m tokens_num  \u001b[38;5;66;03m# 6ND\u001b[39;00m\n",
      "File \u001b[0;32m~/code/gollem/gollem/gpu_stats.py:121\u001b[0m, in \u001b[0;36mget_gpu_flops_for_all_gpus\u001b[0;34m(dtype, ignore_if_unavailable)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m GPU_INFO:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         result[name] \u001b[38;5;241m=\u001b[39m \u001b[43mget_gpu_flops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ignore_if_unavailable:\n",
      "File \u001b[0;32m~/code/gollem/gollem/gpu_stats.py:103\u001b[0m, in \u001b[0;36mget_gpu_flops\u001b[0;34m(name, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gpu_info\u001b[38;5;241m.\u001b[39mtf32_flops\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m gpu_info\u001b[38;5;241m.\u001b[39mbf16_flops \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 FLOPS not available for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gpu_info\u001b[38;5;241m.\u001b[39mbf16_flops\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: BF16 FLOPS not available for RTX4090"
     ]
    }
   ],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "model_size = model_cfg.get_params().total  # this is number of parameters, N\n",
    "\n",
    "# TODO change these parameters\n",
    "tokens_num = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "num_gpus = 8  # number of GPUS used in parallel\n",
    "\n",
    "print(\"Time needed to train model on different GPUS\")\n",
    "print(f\"{'GPU':10s} {'time (days)':16s}\")\n",
    "for gpu_name, gpu_flops in get_gpu_flops_for_all_gpus(dtype).items():\n",
    "    flops_throughput = gpu_flops * num_gpus * assumed_mfu\n",
    "    flops_needed = 6 * model_size * tokens_num  # 6ND\n",
    "    time_needed_s = flops_needed / flops_throughput  # in seconds\n",
    "    print(f\"{gpu_name:10s} {time_needed_s / 3600 / 24:10.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
