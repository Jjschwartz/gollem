{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Sizing\n",
    "\n",
    "This notebook runs a bunch of analysis about a GPT-2 Transformer, e.g. number of FLOPS, parameters, peak memory footprint, checkpoint size, etc\n",
    "\n",
    "**Reference**\n",
    "- This notebook is based directly on Karpathy's [nanoGPT/transformer_sizing.ipynb](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gollem.models.gpt2.config import get_gpt2_model_config\n",
    "from gollem.models.llama3.config import get_llama3_model_config\n",
    "from gollem.gpu_stats import get_gpu_flops_for_all_gpus\n",
    "from gollem.gpu_stats import get_gpu_flops\n",
    "from gollem.gpu_stats import GPU_INFO\n",
    "\n",
    "# NOTE: change things here for your model\n",
    "# ----------------------------------------\n",
    "# Change this to the model you want to analyze\n",
    "model_name = \"gpt2\"\n",
    "# observed checkpoint size\n",
    "measured_checkpoint_size_bytes: int | None = None\n",
    "# measured throughput\n",
    "measured_tokens_per_second = 27026\n",
    "# what GPU we used?\n",
    "gpu_name = \"H100\"\n",
    "# what precision we used? (float32, float16, bfloat16, int8, etc)\n",
    "dtype = \"bfloat16\"\n",
    "# ----------------------------------------\n",
    "\n",
    "if model_name.startswith(\"gpt\"):\n",
    "    model_cfg = get_gpt2_model_config(model_name)\n",
    "elif model_name.startswith(\"llama\"):\n",
    "    model_cfg = get_llama3_model_config(model_name)\n",
    "else:\n",
    "    raise ValueError(f\"Model name {model_name} not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count\n",
    "\n",
    "Here we look at the total number of parameters and the breakdown by component for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 params     ratio (%) \n",
      "embedding/position       786432     0.6322\n",
      "embedding/token        38597376    31.0261\n",
      "embedding              39383808    31.6583\n",
      "attention/ln               1536     0.0012\n",
      "attention/kqv           1769472     1.4224\n",
      "attention/proj           589824     0.4741\n",
      "attention               2360832     1.8977\n",
      "mlp/ln                     1536     0.0012\n",
      "mlp/ffw                 2362368     1.8990\n",
      "mlp/proj                2360064     1.8971\n",
      "mlp                     4723968     3.7973\n",
      "block                   7084800     5.6950\n",
      "transformer            85017600    68.3405\n",
      "ln_f                       1536     0.0012\n",
      "out_embedding                 0     0.0000\n",
      "total                 124402944   100.0000\n"
     ]
    }
   ],
   "source": [
    "model_params = model_cfg.get_params()\n",
    "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "for k, v in model_params.per_component.items():\n",
    "    print(f\"{k:20s} {v:10d} {v / model_params.total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter/Checkpoint Size\n",
    "\n",
    "We can now calculate the size of each checkpoint.\n",
    "\n",
    "Noting that params are stored in fp32 (i.e. 4 bytes), and the AdamW optimizer has 2 additional buffers per param for statistics.\n",
    "\n",
    "We can also compare this to the observed checkpoint size, if we have it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est checkpoint size: 1.49 GB\n"
     ]
    }
   ],
   "source": [
    "params_bytes = model_params.total * 4\n",
    "params_and_buffers_bytes = params_bytes + 2 * params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes / 1e9:.2f} GB\")\n",
    "\n",
    "if measured_checkpoint_size_bytes is not None:\n",
    "    print(f\"measured with wc -c ckpt.pt: {measured_checkpoint_size_bytes}\")\n",
    "    print(\n",
    "        f\"fluff ratio: {measured_checkpoint_size_bytes / params_and_buffers_bytes * 100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU memory usage\n",
    "\n",
    "We can estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer for different GPU sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory ratio taken up just for parameters (incl. optimizer)\n",
      "GPU          ratio (%)\n",
      "H100             1.74\n",
      "A100             1.74\n",
      "RTX4090          5.79\n",
      "RTX3090          5.79\n"
     ]
    }
   ],
   "source": [
    "# Nvidia reports memory in GiB (denoted as GB but is actually GiB)\n",
    "print(\"GPU memory ratio taken up just for parameters (incl. optimizer)\")\n",
    "print(f\"{'GPU':12s} {'ratio (%)':8s}\")\n",
    "for k, v in GPU_INFO.items():\n",
    "    print(f\"{k:12s} {params_and_buffers_bytes / v.memory_bytes * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU forward-backward memory usage\n",
    "\n",
    "We can estimate the total memory usage of the model over the course of a forward-backward pass.\n",
    "\n",
    "This is made up of:\n",
    "\n",
    "1. M_model - the model and optimizer parameters\n",
    "2. M_optimizer - the optimizer buffers\n",
    "3. M_gradient - the gradient of the model\n",
    "4. M_activations - the activations of the model\n",
    "\n",
    "The activations scale with batch size and are typically the largest component of memory usage, since the others are fixed given model size and context length.\n",
    "\n",
    "Calculating the activation memory usage is quite tricky since it is affected by low level optimizations that can be hard to estimate exactly. It is also affected by things like flash attention, dropout, activation checkpointing, etc.\n",
    "\n",
    "Another source of memory usage is the CUDA and pytorch overhead which is typically 0.5-2GiB and will depend on the setup of the system that is running. We don't include this in the calculations, so factor this in when comparing calculated vs empirical memory usage.\n",
    "\n",
    "NOTE: these are estimates, so really should be taken as ballpark figures rather than exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_model: 0.70 GiB\n",
      "M_optimizer: 0.93 GiB\n",
      "M_gradient: 0.46 GiB\n",
      "batch size M_activations total memory\n",
      "         1          0.28         2.37\n",
      "         2          0.57         2.65\n",
      "         4          1.14         3.22\n",
      "         8          2.27         4.36\n",
      "        16          4.55         6.63\n",
      "        32          9.10        11.18\n"
     ]
    }
   ],
   "source": [
    "# when using bf16 or fp16 we use mixed precision so have tostore\n",
    "# both fp16 and fp32 versions of the parameters\n",
    "# when using fp32 we store only the full precision fp32 parameters\n",
    "bytes_per_param = 6 if dtype in (\"bfloat16\", \"float16\") else 4\n",
    "M_model = model_params.total * bytes_per_param\n",
    "\n",
    "# AdamW optimizer has 2 buffers per parameter\n",
    "# values are stored in fp32\n",
    "M_optimizer = 2 * model_params.total * 4\n",
    "\n",
    "# we store one gradient value per parameter\n",
    "# Gradient are stored in fp32\n",
    "M_gradient = model_params.total * 4\n",
    "\n",
    "divisor = 1024**3\n",
    "\n",
    "print(f\"M_model: {M_model / divisor:.2f} GiB\")\n",
    "print(f\"M_optimizer: {M_optimizer / divisor:.2f} GiB\")\n",
    "print(f\"M_gradient: {M_gradient / divisor:.2f} GiB\")\n",
    "\n",
    "# M_activations is more complex\n",
    "print(f\"{'batch size':10s} {'M_activations':13s} {'total memory':12s}\")\n",
    "for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "    M_activations = model_cfg.compute_activations(batch_size, dtype)\n",
    "    total_memory = M_model + M_optimizer + M_activations.total + M_gradient\n",
    "    total_memory_GiB = total_memory / divisor\n",
    "    print(\n",
    "        f\"{batch_size:10d} {M_activations.total / divisor:13.2f} {total_memory_GiB:12.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOPS\n",
    "\n",
    "Now let's looks at the FLOPS used by the model and compare to the estimate from the PaLM paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 flops          ratio (%) \n",
      "attention/kqv            3623878656     1.2426\n",
      "attention/scores         1610612736     0.5522\n",
      "attention/reduce         1610612736     0.5522\n",
      "attention/proj           1207959552     0.4142\n",
      "attention                8053063680     2.7612\n",
      "mlp/ffw1                 4831838208     1.6567\n",
      "mlp/ffw2                 4831838208     1.6567\n",
      "mlp                      9663676416     3.3135\n",
      "block                   17716740096     6.0747\n",
      "transformer            212600881152    72.8963\n",
      "out_embedding           79047426048    27.1037\n",
      "forward_total          291648307200   100.0000\n",
      "backward_total         583296614400   200.0000\n",
      "total                  874944921600   300.0000\n"
     ]
    }
   ],
   "source": [
    "model_flops = model_cfg.compute_flops()\n",
    "flops_total = model_flops.forward_total\n",
    "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
    "for k, v in model_flops.per_component.items():\n",
    "    print(f\"{k:20s} {v:14d} {v / flops_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFU\n",
    "\n",
    "Given our estimated FLOPS we can calculate how much of our GPU's FLOP capacity is being used, that is our model flop utilization (MFU).\n",
    "\n",
    "To calculate this we need 3 bits of information:\n",
    "\n",
    "- GPU speed (FLOPS)\n",
    "- observer throughput (tokens per second)\n",
    "- model flops (FLOPs) per iteration\n",
    "\n",
    "For the GPU speed we refer to: https://www.techpowerup.com/gpu-specs/ which gives theoretical performance, specifically looking at performance for BF16 and FP16, which ever is supported and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_flops_per_token 854438400.0\n",
      "gpu_flops: 9.89e+14\n",
      "GPU        MFU (%) \n",
      "H100           2.33\n"
     ]
    }
   ],
   "source": [
    "# calculate flops achieved\n",
    "# Get model flops per token (note we need to divide by context length\n",
    "# to get the flops per token)\n",
    "model_flops_per_token = model_cfg.compute_flops().total / model_cfg.n_ctx\n",
    "gpu_flops = get_gpu_flops(gpu_name, dtype)\n",
    "\n",
    "print(\"model_flops_per_token\", model_flops_per_token)\n",
    "print(f\"gpu_flops: {gpu_flops:.2e}\")\n",
    "\n",
    "print(f\"{'GPU':10s} {'MFU (%)':8s}\")\n",
    "mfu = (model_flops_per_token * measured_tokens_per_second) / gpu_flops\n",
    "print(f\"{gpu_name:10s} {mfu * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Training Compute\n",
    "\n",
    "Here we use the value computed so far to compute an estimate of the total amount of compute needed to train the model.\n",
    "\n",
    "Here we estimate the total amount of compute `C` required to train the model as `C ~= 6*N*D`, where:\n",
    "\n",
    "- `6` is a heuristic value (see [Dzmitry Bahdanau's post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) for an explanation) \n",
    "    - it basically stems from weight multiplications requiring 6 FLOPS per token per weight when combining both forward and backward passes.\n",
    "- `N` is the total number of model parameters\n",
    "- `D` is total size of dataset (in tokens)\n",
    "\n",
    "Note the equation changes to `C ~= 8*N*D` with activation checkpointing (i.e. recompute activations as needed to save memory when doing back-prop).\n",
    "\n",
    "We also need to factor in the model flops utilization (MFU) to correct for the fact that we cannot use 100% of the GPUs FLOPS due to memory bottlenecks, etc (again see Dzmitry's blog for examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to train model on different GPUS\n",
      "GPU        time (days)     \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "BF16 FLOPS not available for RTX4090",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime needed to train model on different GPUS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime (days)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m16s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gpu_name, gpu_flops \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_gpu_flops_for_all_gpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     12\u001b[0m     flops_throughput \u001b[38;5;241m=\u001b[39m gpu_flops \u001b[38;5;241m*\u001b[39m num_gpus \u001b[38;5;241m*\u001b[39m assumed_mfu\n\u001b[1;32m     13\u001b[0m     flops_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m model_size \u001b[38;5;241m*\u001b[39m tokens_num  \u001b[38;5;66;03m# 6ND\u001b[39;00m\n",
      "File \u001b[0;32m~/code/gollem/gollem/gpu_stats.py:121\u001b[0m, in \u001b[0;36mget_gpu_flops_for_all_gpus\u001b[0;34m(dtype, ignore_if_unavailable)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m GPU_INFO:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         result[name] \u001b[38;5;241m=\u001b[39m \u001b[43mget_gpu_flops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ignore_if_unavailable:\n",
      "File \u001b[0;32m~/code/gollem/gollem/gpu_stats.py:103\u001b[0m, in \u001b[0;36mget_gpu_flops\u001b[0;34m(name, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gpu_info\u001b[38;5;241m.\u001b[39mtf32_flops\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m gpu_info\u001b[38;5;241m.\u001b[39mbf16_flops \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 FLOPS not available for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gpu_info\u001b[38;5;241m.\u001b[39mbf16_flops\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: BF16 FLOPS not available for RTX4090"
     ]
    }
   ],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "model_size = model_cfg.get_params().total  # this is number of parameters, N\n",
    "\n",
    "# TODO change these parameters\n",
    "tokens_num = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "num_gpus = 8  # number of GPUS used in parallel\n",
    "\n",
    "print(\"Time needed to train model on different GPUS\")\n",
    "print(f\"{'GPU':10s} {'time (days)':16s}\")\n",
    "for gpu_name, gpu_flops in get_gpu_flops_for_all_gpus(dtype).items():\n",
    "    flops_throughput = gpu_flops * num_gpus * assumed_mfu\n",
    "    flops_needed = 6 * model_size * tokens_num  # 6ND\n",
    "    time_needed_s = flops_needed / flops_throughput  # in seconds\n",
    "    print(f\"{gpu_name:10s} {time_needed_s / 3600 / 24:10.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
