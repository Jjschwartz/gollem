{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Sizing\n",
    "\n",
    "This notebook runs a bunch of analysis about a GPT-2 Transformer, e.g. number of FLOPS, parameters, peak memory footprint, checkpoint size, etc\n",
    "\n",
    "**Reference**\n",
    "- This notebook is based directly on Karpathy's [nanoGPT/transformer_sizing.ipynb](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_ctx: int = 1024\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    d_model: int = 768\n",
    "    d_mlp: int = 4 * 768\n",
    "    vocab_size: int = 50257\n",
    "    ln_bias: bool = False\n",
    "    mlp_bias: bool = False\n",
    "    share_embd_params: bool = True\n",
    "\n",
    "\n",
    "MODEL_CONFIG_ARGS = {\n",
    "    # 14M params\n",
    "    \"gpt2-tiny\": ModelConfig(\n",
    "        n_ctx=128,\n",
    "        n_layer=2,\n",
    "        n_head=4,\n",
    "        d_model=256,\n",
    "        d_mlp=4 * 256,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 124M params\n",
    "    \"gpt2\": ModelConfig(\n",
    "        n_ctx=1024,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        d_model=768,\n",
    "        d_mlp=4 * 768,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 350M params\n",
    "    \"gpt2-medium\": ModelConfig(\n",
    "        n_ctx=1024,\n",
    "        n_layer=24,\n",
    "        n_head=16,\n",
    "        d_model=1024,\n",
    "        d_mlp=4 * 1024,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 774M params\n",
    "    \"gpt2-large\": ModelConfig(\n",
    "        n_ctx=1024,\n",
    "        n_layer=36,\n",
    "        n_head=20,\n",
    "        d_model=1280,\n",
    "        d_mlp=4 * 1280,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 1558M params\n",
    "    \"gpt2-xl\": ModelConfig(\n",
    "        n_ctx=1024,\n",
    "        n_layer=48,\n",
    "        n_head=25,\n",
    "        d_model=1600,\n",
    "        d_mlp=4 * 1600,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def load_config(name: str) -> ModelConfig:\n",
    "    assert name in MODEL_CONFIG_ARGS\n",
    "    return MODEL_CONFIG_ARGS[name]\n",
    "\n",
    "\n",
    "# TODO Change this as desired\n",
    "model_cfg = load_config(\"gpt2-tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we see: 14476544, expected: 124402944, match: False\n",
      "name                 params     ratio (%) \n",
      "embedding/position        32768     0.2264\n",
      "embedding/token        12865792    88.8734\n",
      "embedding              12898560    89.0997\n",
      "attention/ln                512     0.0035\n",
      "attention/kqv            196608     1.3581\n",
      "attention/proj            65536     0.4527\n",
      "attention                262656     1.8144\n",
      "mlp/ln                      512     0.0035\n",
      "mlp/ffw                  263168     1.8179\n",
      "mlp/proj                 262400     1.8126\n",
      "mlp                      526080     3.6340\n",
      "block                    788736     5.4484\n",
      "transformer             1577472    10.8967\n",
      "ln_f                        512     0.0035\n",
      "out_embedding                 0     0.0000\n",
      "total                  14476544   100.0000\n"
     ]
    }
   ],
   "source": [
    "def get_params(cfg: ModelConfig):\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = cfg.n_ctx * cfg.d_model\n",
    "    out[\"embedding/token\"] = cfg.vocab_size * cfg.d_model\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "    out[\"attention/kqv\"] = cfg.d_model * 3 * cfg.d_model\n",
    "    out[\"attention/proj\"] = cfg.d_model**2\n",
    "    out[\"attention\"] = (\n",
    "        out[\"attention/ln\"] + out[\"attention/kqv\"] + out[\"attention/proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/ln\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "    out[\"mlp/ffw\"] = cfg.d_model * cfg.d_mlp + int(cfg.ln_bias) * cfg.d_mlp\n",
    "    out[\"mlp/proj\"] = cfg.d_mlp * cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "    out[\"mlp\"] = out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/proj\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "    out[\"ln_f\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model  # final layernorm\n",
    "    if cfg.share_embd_params:\n",
    "        # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "        out[\"out_embedding\"] = 0\n",
    "    else:\n",
    "        out[\"out_embedding\"] = cfg.d_model * cfg.vocab_size\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = (\n",
    "        out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"out_embedding\"]\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# compare our param count to that reported by PyTorch (for \"GPT2\" with 124M params)\n",
    "# TODO update the PyTorch value to include bias\n",
    "model_params = get_params(model_cfg)\n",
    "params_total = model_params[\"total\"]\n",
    "print(\n",
    "    f\"we see: {params_total}, expected: {124402944}, match: {params_total == 124337664}\"\n",
    ")\n",
    "# create a header\n",
    "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "for k, v in model_params.items():\n",
    "    print(f\"{k:20s} {v:10d} {v / params_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter/Checkpoint Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est checkpoint size: 0.17 GB\n",
      "measured with wc -c ckpt.pt: 1542470366\n",
      "fluff ratio: 887.91%\n"
     ]
    }
   ],
   "source": [
    "# we can now calculate the size of each checkpoint\n",
    "# params are stored in fp32 (i.e. 4 bytes), and the AdamW optimizer has 2 additional buffers per param for statistics\n",
    "params_bytes = params_total * 4\n",
    "params_and_buffers_bytes = params_bytes + 2 * params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes / 1e9:.2f} GB\")\n",
    "# TODO update this with actual measured bytes\n",
    "measured_bytes = 1542470366  # from wc -c ckpt.pt\n",
    "print(f\"measured with wc -c ckpt.pt: {measured_bytes}\")\n",
    "print(f\"fluff ratio: {measured_bytes / params_and_buffers_bytes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU memory usage\n",
    "\n",
    "We can estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory ratio taken up just for parameters (incl. optimizer)\n",
      "GPU          ratio (%)\n",
      "H100             0.20\n",
      "A100             0.40\n",
      "RTX4090          0.67\n",
      "RTX2070          2.02\n"
     ]
    }
   ],
   "source": [
    "# Nvidia reports memory in GiB (denoted as GB but is actually GiB)\n",
    "# 1 GiB = 1024 MiB = 1024**2 KiB = 1024**3 Bytes\n",
    "GPU_MEMORY = {\n",
    "    \"H100\": 80 * 1024**3,  # 80 GiB\n",
    "    \"A100\": 40 * 1024**3,  # 40 GiB\n",
    "    \"RTX4090\": 24 * 1024**3,  # 24 GiB\n",
    "    \"RTX2070\": 8 * 1024**3,  # 8 GiB\n",
    "}\n",
    "\n",
    "print(\"GPU memory ratio taken up just for parameters (incl. optimizer)\")\n",
    "print(f\"{'GPU':12s} {'ratio (%)':8s}\")\n",
    "for k, v in GPU_MEMORY.items():\n",
    "    print(f\"{k:12s} {params_and_buffers_bytes / v * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOPS\n",
    "\n",
    "Here we estimate FLOPS for a single forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 flops          ratio (%) \n",
      "attention/kqv              50331648     1.3494\n",
      "attention/scores            8388608     0.2249\n",
      "attention/reduce            8388608     0.2249\n",
      "attention/proj             16777216     0.4498\n",
      "attention                  83886080     2.2490\n",
      "mlp/ffw1                   67108864     1.7992\n",
      "mlp/ffw2                   67108864     1.7992\n",
      "mlp                       134217728     3.5985\n",
      "block                     218103808     5.8475\n",
      "transformer               436207616    11.6950\n",
      "out_embedding            3293642752    88.3050\n",
      "forward_total            3729850368   100.0000\n",
      "backward_total           7459700736   200.0000\n",
      "total                   11189551104   300.0000\n"
     ]
    }
   ],
   "source": [
    "def compute_flops(cfg: ModelConfig):\n",
    "    # we only count Weight FLOPs,\n",
    "    # FLOPS for all other layers (LayerNorm, Softmax, etc) and bias vector additian are effectively irrelevant\n",
    "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
    "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    head_size = cfg.d_model // cfg.n_head\n",
    "\n",
    "    # attention blocks\n",
    "    # 1) the projection to key, query, values\n",
    "    out[\"attention/kqv\"] = 2 * cfg.n_ctx * (cfg.d_model * 3 * cfg.d_model)\n",
    "    # 2) calculating the attention scores\n",
    "    out[\"attention/scores\"] = 2 * cfg.n_ctx * cfg.n_ctx * cfg.d_model\n",
    "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out[\"attention/reduce\"] = 2 * cfg.n_head * (cfg.n_ctx * cfg.n_ctx * head_size)\n",
    "    # 4) the final linear projection\n",
    "    out[\"attention/proj\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.d_model)\n",
    "    out[\"attention\"] = sum(\n",
    "        out[\"attention/\" + k] for k in [\"kqv\", \"scores\", \"reduce\", \"proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/ffw1\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.d_mlp)\n",
    "    out[\"mlp/ffw2\"] = 2 * cfg.n_ctx * (cfg.d_mlp * cfg.d_model)\n",
    "    out[\"mlp\"] = out[\"mlp/ffw1\"] + out[\"mlp/ffw2\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "    out[\"out_embedding\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.vocab_size)\n",
    "\n",
    "    # forward,backward,total\n",
    "    out[\"forward_total\"] = out[\"transformer\"] + out[\"out_embedding\"]\n",
    "    out[\"backward_total\"] = (\n",
    "        2 * out[\"forward_total\"]\n",
    "    )  # use common estimate of bwd = 2*fwd\n",
    "    out[\"total\"] = out[\"forward_total\"] + out[\"backward_total\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# compare our param count to that reported by PyTorch\n",
    "model_flops = compute_flops(model_cfg)\n",
    "flops_total = model_flops[\"forward_total\"]\n",
    "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
    "for k, v in model_flops.items():\n",
    "    print(f\"{k:20s} {v:14d} {v / flops_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palm_flops: 11193483264, flops: 11189551104, ratio: 1.0004\n"
     ]
    }
   ],
   "source": [
    "# now here is an estimate copy pasted from the PaLM paper\n",
    "# this formula is often used to calculate MFU (model flops utilization)\n",
    "def compute_palm_flops(cfg: ModelConfig):\n",
    "    \"\"\"estimate of the model flops following PaLM paper formula\"\"\"\n",
    "    # non-embedding model parameters. note that we do not subtract the\n",
    "    # embedding/token params because those are tied and get used in the last layer.\n",
    "    model_params = get_params(cfg)\n",
    "    N = model_params[\"total\"] - model_params[\"embedding/position\"]\n",
    "    L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.d_model // cfg.n_head, cfg.n_ctx\n",
    "    mf_per_token = 6 * N + 12 * L * H * Q * T\n",
    "    mf = mf_per_token * cfg.n_ctx\n",
    "    return mf\n",
    "\n",
    "\n",
    "palm_flops = compute_palm_flops(model_cfg)\n",
    "print(\n",
    "    f\"palm_flops: {palm_flops:d}, flops: {model_flops['total']:d}, ratio: {palm_flops / model_flops['total']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Flops Usage\n",
    "\n",
    "Given our estimated FLOPS we can calculate how much of our GPU FLOP capacity is being used, that is our model flop utilization (MFU).\n",
    "\n",
    "To calculate this we need a few bits of information:\n",
    "\n",
    "- GPU speed (FLOPS)\n",
    "- batch size (including gradient accumulation)\n",
    "- time per iteration\n",
    "\n",
    "For the GPU speed we refer to: https://www.techpowerup.com/gpu-specs/ which gives theoretical performance, specifically looking at performance for BF16 and FP16, which ever is supported and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of GPU FLOPS used\n",
      "GPU            ratio (%) \n",
      "H100                 0.20\n",
      "A100                 0.48\n",
      "RTX4090              1.79\n",
      "RTX2070              9.88\n"
     ]
    }
   ],
   "source": [
    "# R\n",
    "GPU_FLOPS = {\n",
    "    \"H100\": 756e12,  # 756 TFLOPS BF16 (this is a guess, spec sheet shows 1513 for sparse tensors)\n",
    "    \"A100\": 312e12,  # 312 TFLOPS BF16\n",
    "    \"RTX4090\": 83e12,  # 83 TFLOPS FP16\n",
    "    \"RTX2070\": 15e12,  # 15 TFLOPS FP16\n",
    "}\n",
    "\n",
    "# TODO Change these values to desired values\n",
    "batch_size = 20\n",
    "grad_accum = 5\n",
    "measured_time = 0.755  # in seconds per iteration\n",
    "\n",
    "# calculate flops achieved\n",
    "total_batch_size = batch_size * grad_accum\n",
    "measured_throughput = total_batch_size / measured_time\n",
    "flops_achieved = model_flops[\"total\"] * measured_throughput\n",
    "\n",
    "# the fraction of the A100 that we are using:\n",
    "print(\"Fraction of GPU FLOPS used\")\n",
    "print(f\"{'GPU':14s} {'ratio (%)':10s}\")\n",
    "for k, v in GPU_FLOPS.items():\n",
    "    print(f\"{k:14s} {flops_achieved / v * 100:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Training Compute\n",
    "\n",
    "Here we use the value computed so far to compute an estimate of the total amount of compute needed to train the model.\n",
    "\n",
    "Here we estimate the total amount of compute `C` required to train the model as `C ~= 6*N*D`, where:\n",
    "\n",
    "- `6` is a heuristic value (see [Dzmitry Bahdanau's post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) for an explanation) \n",
    "    - it basically stems from weight multiplications requiring 6 FLOPS per token per weight when combining both forward and backward passes.\n",
    "- `N` is the total number of model parameters\n",
    "- `D` is total size of dataset (in tokens)\n",
    "\n",
    "Note the equation changes to `C ~= 8*N*D` with activation checkpointing (i.e. recompute activations as needed to save memory when doing back-prop).\n",
    "\n",
    "We also need to factor in the model flops utilization (MFU) to correct for the fact that we cannot use 100% of the GPUs FLOPS due to memory bottlenecks, etc (again see Dzmitry's blog for examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to train model on different GPUS\n",
      "GPU        time (days)     \n",
      "H100             0.17\n",
      "A100             0.40\n",
      "RTX4090          1.51\n",
      "RTX2070          8.38\n"
     ]
    }
   ],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "model_size = get_params(model_cfg)[\"total\"]  # this is number of parameters, N\n",
    "\n",
    "# TODO change these parameters\n",
    "tokens_num = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "num_gpus = 8  # number of GPUS used in parallel\n",
    "\n",
    "print(\"Time needed to train model on different GPUS\")\n",
    "print(f\"{'GPU':10s} {'time (days)':16s}\")\n",
    "for gpu_name, gpu_flops in GPU_FLOPS.items():\n",
    "    flops_throughput = gpu_flops * num_gpus * assumed_mfu\n",
    "    flops_needed = 6 * model_size * tokens_num  # 6ND\n",
    "    time_needed_s = flops_needed / flops_throughput  # in seconds\n",
    "    print(f\"{gpu_name:10s} {time_needed_s / 3600 / 24:10.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
