{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Sizing\n",
    "\n",
    "This notebook runs a bunch of analysis about a GPT-2 Transformer, e.g. number of FLOPS, parameters, peak memory footprint, checkpoint size, etc\n",
    "\n",
    "**Reference**\n",
    "- This notebook is based directly on Karpathy's [nanoGPT/transformer_sizing.ipynb](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathon/code/gollem/gollem/models/gpt2/model.py:18: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from gollem.models.gpt2.config import GPT2Config\n",
    "from gollem.models.gpt2.config import get_gpt2_model_config\n",
    "\n",
    "# Change this to the model you want to analyze\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "model_cfg = get_gpt2_model_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we see: 124402944, expected: 124402944, match: True\n",
      "name                 params     ratio (%) \n",
      "embedding/position       786432     0.6322\n",
      "embedding/token        38597376    31.0261\n",
      "embedding              39383808    31.6583\n",
      "attention/ln               1536     0.0012\n",
      "attention/kqv           1769472     1.4224\n",
      "attention/proj           589824     0.4741\n",
      "attention               2360832     1.8977\n",
      "mlp/ln                     1536     0.0012\n",
      "mlp/ffw                 2362368     1.8990\n",
      "mlp/proj                2360064     1.8971\n",
      "mlp                     4723968     3.7973\n",
      "block                   7084800     5.6950\n",
      "transformer            85017600    68.3405\n",
      "ln_f                       1536     0.0012\n",
      "out_embedding                 0     0.0000\n",
      "total                 124402944   100.0000\n"
     ]
    }
   ],
   "source": [
    "def get_params(cfg: GPT2Config):\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = cfg.n_ctx * cfg.d_model\n",
    "    out[\"embedding/token\"] = cfg.vocab_size * cfg.d_model\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "    out[\"attention/kqv\"] = cfg.d_model * 3 * cfg.d_model\n",
    "    out[\"attention/proj\"] = cfg.d_model**2\n",
    "    out[\"attention\"] = (\n",
    "        out[\"attention/ln\"] + out[\"attention/kqv\"] + out[\"attention/proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/ln\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "    out[\"mlp/ffw\"] = cfg.d_model * cfg.d_mlp + int(cfg.ln_bias) * cfg.d_mlp\n",
    "    out[\"mlp/proj\"] = cfg.d_mlp * cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "    out[\"mlp\"] = out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/proj\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "    out[\"ln_f\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model  # final layernorm\n",
    "    if cfg.share_embd_params:\n",
    "        # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "        out[\"out_embedding\"] = 0\n",
    "    else:\n",
    "        out[\"out_embedding\"] = cfg.d_model * cfg.vocab_size\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = (\n",
    "        out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"out_embedding\"]\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# compare our param count to that reported by PyTorch (for \"GPT2\" with 124M params)\n",
    "# TODO update the PyTorch value to include bias\n",
    "model_params = get_params(model_cfg)\n",
    "params_total = model_params[\"total\"]\n",
    "if model_cfg.model_name == \"gpt2\":\n",
    "    expected_params = 124402944\n",
    "    print(\n",
    "        f\"we see: {params_total}, expected: {expected_params}, match: {params_total == expected_params}\"\n",
    "    )\n",
    "# create a header\n",
    "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "for k, v in model_params.items():\n",
    "    print(f\"{k:20s} {v:10d} {v / params_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter/Checkpoint Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est checkpoint size: 1.49 GB\n",
      "measured with wc -c ckpt.pt: 1542470366\n",
      "fluff ratio: 103.32%\n"
     ]
    }
   ],
   "source": [
    "# we can now calculate the size of each checkpoint\n",
    "# params are stored in fp32 (i.e. 4 bytes), and the AdamW optimizer has 2 additional buffers per param for statistics\n",
    "params_bytes = params_total * 4\n",
    "params_and_buffers_bytes = params_bytes + 2 * params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes / 1e9:.2f} GB\")\n",
    "# TODO update this with actual measured bytes\n",
    "measured_bytes = 1542470366  # from wc -c ckpt.pt\n",
    "print(f\"measured with wc -c ckpt.pt: {measured_bytes}\")\n",
    "print(f\"fluff ratio: {measured_bytes / params_and_buffers_bytes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU memory usage\n",
    "\n",
    "We can estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory ratio taken up just for parameters (incl. optimizer)\n",
      "GPU          ratio (%)\n",
      "H100             1.74\n",
      "A100             3.48\n",
      "RTX3090          5.79\n",
      "RTX4090          5.79\n",
      "RTX2070         17.38\n"
     ]
    }
   ],
   "source": [
    "# Nvidia reports memory in GiB (denoted as GB but is actually GiB)\n",
    "# 1 GiB = 1024 MiB = 1024**2 KiB = 1024**3 Bytes\n",
    "GPU_MEMORY = {\n",
    "    \"H100\": 80 * 1024**3,  # 80 GiB\n",
    "    \"A100\": 40 * 1024**3,  # 40 GiB\n",
    "    \"RTX3090\": 24 * 1024**3,  # 24 GiB\n",
    "    \"RTX4090\": 24 * 1024**3,  # 24 GiB\n",
    "    \"RTX2070\": 8 * 1024**3,  # 8 GiB\n",
    "}\n",
    "\n",
    "print(\"GPU memory ratio taken up just for parameters (incl. optimizer)\")\n",
    "print(f\"{'GPU':12s} {'ratio (%)':8s}\")\n",
    "for k, v in GPU_MEMORY.items():\n",
    "    print(f\"{k:12s} {params_and_buffers_bytes / v * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU forward-backward memory usage\n",
    "\n",
    "We can estimate the total memory usage of the model over the course of a forward-backward pass.\n",
    "\n",
    "This is made up of:\n",
    "\n",
    "1. M_model - the model and optimizer parameters\n",
    "2. M_optimizer - the optimizer buffers\n",
    "3. M_gradient - the gradient of the model\n",
    "4. M_activations - the activations of the model\n",
    "\n",
    "The activations scale with batch size and are typically the largest component of memory usage, since the others are fixed given model size and context length.\n",
    "\n",
    "Calculating the activation memory usage is quite tricky since it is affected by low level optimizations that can be hard to estimate exactly. It is also affected by things like flash attention, dropout, activation checkpointing, etc.\n",
    "\n",
    "Another source of memory usage is the CUDA and pytorch overhead which is typically 0.5-2GiB and will depend on the setup of the system that is running. We don't include this in the calculations, so factor this in when comparing calculated vs empirical memory usage.\n",
    "\n",
    "NOTE: these are estimates, so really should be taken as ballpark figures rather than exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_activations(cfg: GPT2Config, B: int, dtype: str, using_flash_attn: bool):\n",
    "    # Total: $8B + 16T + L \\times (34TBH + 5AT^2B) + 4TBH$\n",
    "    out = OrderedDict()\n",
    "\n",
    "    bytes_per_activation = 2 if dtype in [\"bfloat16\", \"float16\"] else 4\n",
    "    bytes_per_long = 8\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = cfg.n_ctx * bytes_per_long\n",
    "    out[\"embedding/token\"] = cfg.n_ctx * B * bytes_per_long\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    TBH = cfg.n_ctx * B * cfg.d_model\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = TBH * bytes_per_activation\n",
    "    out[\"attention/kqv\"] = TBH * bytes_per_activation\n",
    "    if using_flash_attn:\n",
    "        # when using flash attention a bunch of optimizations are done\n",
    "        out[\"attention/qk_matmul\"] = 0\n",
    "        out[\"attention/softmax\"] = 0\n",
    "        # flash attention requires K, Q, V as well as two vectors l, m of length T (size BT)\n",
    "        out[\"attention/attention_over_v\"] = (\n",
    "            TBH * 3 + 2 * cfg.n_ctx * B\n",
    "        ) * bytes_per_activation\n",
    "    else:\n",
    "        out[\"attention/qk_matmul\"] = TBH * 2 * bytes_per_activation\n",
    "        out[\"attention/softmax\"] = cfg.n_head * cfg.n_ctx**2 * B * bytes_per_activation\n",
    "        out[\"attention/attention_over_v\"] = (\n",
    "            TBH + cfg.n_head * cfg.n_ctx**2 * B\n",
    "        ) * bytes_per_activation\n",
    "    out[\"attention/proj\"] = TBH * bytes_per_activation\n",
    "    out[\"attention\"] = (\n",
    "        out[\"attention/ln\"]\n",
    "        + out[\"attention/kqv\"]\n",
    "        + out[\"attention/qk_matmul\"]\n",
    "        + out[\"attention/softmax\"]\n",
    "        + out[\"attention/attention_over_v\"]\n",
    "        + out[\"attention/proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/ln\"] = TBH * bytes_per_activation\n",
    "    out[\"mlp/ffw\"] = TBH * bytes_per_activation\n",
    "    out[\"mlp/ffw_activation\"] = cfg.n_ctx * B * cfg.d_mlp * bytes_per_activation\n",
    "    out[\"mlp/proj\"] = cfg.n_ctx * B * cfg.d_mlp * bytes_per_activation\n",
    "    out[\"mlp\"] = (\n",
    "        out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/ffw_activation\"] + out[\"mlp/proj\"]\n",
    "    )\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "\n",
    "    # final layernorm and output projection\n",
    "    out[\"ln_f\"] = TBH * bytes_per_activation\n",
    "    out[\"out_embedding\"] = TBH * bytes_per_activation\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = (\n",
    "        out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"out_embedding\"]\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_model: 0.70 GiB\n",
      "M_optimizer: 0.93 GiB\n",
      "M_gradient: 0.46 GiB\n",
      "batch size M_activations total memory   diff\n",
      "         1          0.28         2.37  -0.15\n",
      "         2          0.57         2.65   0.15\n",
      "         4          1.14         3.22   0.60\n",
      "         8          2.27         4.36   1.51\n",
      "        16          4.55         6.63   3.33\n",
      "        32          9.10        11.18   6.98\n"
     ]
    }
   ],
   "source": [
    "dtype = \"bfloat16\"\n",
    "using_flash_attn = True\n",
    "# dtype = \"float32\"\n",
    "\n",
    "# when using bf16 or fp16 we use mixed precision so have tostore\n",
    "# both fp16 and fp32 versions of the parameters\n",
    "# when using fp32 we store only the full precision fp32 parameters\n",
    "bytes_per_param = 6 if dtype in (\"bfloat16\", \"float16\") else 4\n",
    "M_model = params_total * bytes_per_param\n",
    "\n",
    "# AdamW optimizer has 2 buffers per parameter\n",
    "# values are stored in fp32\n",
    "M_optimizer = 2 * params_total * 4\n",
    "\n",
    "# we store one gradient value per parameter\n",
    "# Gradient are stored in fp32\n",
    "M_gradient = params_total * 4\n",
    "\n",
    "\n",
    "divisor = 1024**3\n",
    "\n",
    "print(f\"M_model: {M_model / divisor:.2f} GiB\")\n",
    "print(f\"M_optimizer: {M_optimizer / divisor:.2f} GiB\")\n",
    "print(f\"M_gradient: {M_gradient / divisor:.2f} GiB\")\n",
    "\n",
    "\n",
    "# Empirical peak memory usage for gpt2 (124M) using flash attention\n",
    "# run_name\t            \t peak_mem_usage\n",
    "# batch_size=1024 (1x1024)\t 2269\n",
    "# batch_size=2048 (2x1024)\t 2870\n",
    "# batch_size=4096 (4x1024)\t 3918\n",
    "# batch_size=8192 (8x1024)\t 6012\n",
    "# batch_size=16384 (16x1024) 10206\n",
    "# batch_size=32768 (32x1024) 18595\n",
    "# Map from batch size to peak memory usage in MiB\n",
    "empirical_peak_memory_usage = {\n",
    "    1: 2269,\n",
    "    2: 2870,\n",
    "    4: 3918,\n",
    "    8: 6012,\n",
    "    16: 10206,\n",
    "    32: 18595,\n",
    "}\n",
    "\n",
    "# M_activations is more complex\n",
    "print(f\"{'batch size':10s} {'M_activations':13s} {'total memory':12s} {'  diff':6s}\")\n",
    "for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "    M_activations = compute_activations(model_cfg, batch_size, dtype, using_flash_attn)\n",
    "    total_memory = M_model + M_optimizer + M_activations[\"total\"] + M_gradient\n",
    "    total_memory_GiB = total_memory / divisor\n",
    "    diff = (empirical_peak_memory_usage[batch_size] / 1024) - total_memory_GiB\n",
    "    print(\n",
    "        f\"{batch_size:10d} {M_activations['total'] / divisor:13.2f} {total_memory_GiB:12.2f} {diff:6.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOPS\n",
    "\n",
    "Here we estimate FLOPS for a single forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 flops          ratio (%) \n",
      "attention/kqv            3623878656     1.2426\n",
      "attention/scores         1610612736     0.5522\n",
      "attention/reduce         1610612736     0.5522\n",
      "attention/proj           1207959552     0.4142\n",
      "attention                8053063680     2.7612\n",
      "mlp/ffw1                 4831838208     1.6567\n",
      "mlp/ffw2                 4831838208     1.6567\n",
      "mlp                      9663676416     3.3135\n",
      "block                   17716740096     6.0747\n",
      "transformer            212600881152    72.8963\n",
      "out_embedding           79047426048    27.1037\n",
      "forward_total          291648307200   100.0000\n",
      "backward_total         583296614400   200.0000\n",
      "total                  874944921600   300.0000\n"
     ]
    }
   ],
   "source": [
    "def compute_flops(cfg: GPT2Config):\n",
    "    # we only count Weight FLOPs,\n",
    "    # FLOPS for all other layers (LayerNorm, Softmax, etc) and bias vector additian are effectively irrelevant\n",
    "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
    "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    head_size = cfg.d_model // cfg.n_head\n",
    "\n",
    "    # attention blocks\n",
    "    # 1) the projection to key, query, values\n",
    "    out[\"attention/kqv\"] = 2 * cfg.n_ctx * (cfg.d_model * 3 * cfg.d_model)\n",
    "    # 2) calculating the attention scores\n",
    "    out[\"attention/scores\"] = 2 * cfg.n_ctx * cfg.n_ctx * cfg.d_model\n",
    "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out[\"attention/reduce\"] = 2 * cfg.n_head * (cfg.n_ctx * cfg.n_ctx * head_size)\n",
    "    # 4) the final linear projection\n",
    "    out[\"attention/proj\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.d_model)\n",
    "    out[\"attention\"] = sum(\n",
    "        out[\"attention/\" + k] for k in [\"kqv\", \"scores\", \"reduce\", \"proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/ffw1\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.d_mlp)\n",
    "    out[\"mlp/ffw2\"] = 2 * cfg.n_ctx * (cfg.d_mlp * cfg.d_model)\n",
    "    out[\"mlp\"] = out[\"mlp/ffw1\"] + out[\"mlp/ffw2\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "    out[\"out_embedding\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.vocab_size)\n",
    "\n",
    "    # forward,backward,total\n",
    "    out[\"forward_total\"] = out[\"transformer\"] + out[\"out_embedding\"]\n",
    "    out[\"backward_total\"] = (\n",
    "        2 * out[\"forward_total\"]\n",
    "    )  # use common estimate of bwd = 2*fwd\n",
    "    out[\"total\"] = out[\"forward_total\"] + out[\"backward_total\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# compare our param count to that reported by PyTorch\n",
    "model_flops = compute_flops(model_cfg)\n",
    "flops_total = model_flops[\"forward_total\"]\n",
    "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
    "for k, v in model_flops.items():\n",
    "    print(f\"{k:20s} {v:14d} {v / flops_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palm_flops: 875463966720, flops: 874944921600, ratio: 1.0006\n"
     ]
    }
   ],
   "source": [
    "# now here is an estimate copy pasted from the PaLM paper\n",
    "# this formula is often used to calculate MFU (model flops utilization)\n",
    "def compute_palm_flops(cfg: GPT2Config):\n",
    "    \"\"\"estimate of the model flops following PaLM paper formula\"\"\"\n",
    "    # non-embedding model parameters. note that we do not subtract the\n",
    "    # embedding/token params because those are tied and get used in the last layer.\n",
    "    model_params = get_params(cfg)\n",
    "    N = model_params[\"total\"] - model_params[\"embedding/position\"]\n",
    "    L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.d_model // cfg.n_head, cfg.n_ctx\n",
    "    mf_per_token = 6 * N + 12 * L * H * Q * T\n",
    "    mf = mf_per_token * cfg.n_ctx\n",
    "    return mf\n",
    "\n",
    "\n",
    "palm_flops = compute_palm_flops(model_cfg)\n",
    "print(\n",
    "    f\"palm_flops: {palm_flops:d}, flops: {model_flops['total']:d}, ratio: {palm_flops / model_flops['total']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Flops Usage\n",
    "\n",
    "Given our estimated FLOPS we can calculate how much of our GPU FLOP capacity is being used, that is our model flop utilization (MFU).\n",
    "\n",
    "To calculate this we need a few bits of information:\n",
    "\n",
    "- GPU speed (FLOPS)\n",
    "- batch size (including gradient accumulation)\n",
    "- time per iteration\n",
    "\n",
    "For the GPU speed we refer to: https://www.techpowerup.com/gpu-specs/ which gives theoretical performance, specifically looking at performance for BF16 and FP16, which ever is supported and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854438400\n",
      "132.4503311258278\n",
      "Fraction of GPU FLOPS used\n",
      "GPU            ratio (%) \n",
      "H100 756000000000000.0\n",
      "H100                15.33\n",
      "A100 312000000000000.0\n",
      "A100                37.14\n",
      "RTX4090 83000000000000.0\n",
      "RTX4090            139.62\n",
      "RTX2070 15000000000000.0\n",
      "RTX2070            772.58\n"
     ]
    }
   ],
   "source": [
    "# R\n",
    "GPU_FLOPS = {\n",
    "    \"H100\": 756e12,  # 756 TFLOPS BF16 (this is a guess, spec sheet shows 1513 for sparse tensors)\n",
    "    \"A100\": 312e12,  # 312 TFLOPS BF16\n",
    "    \"RTX4090\": 83e12,  # 83 TFLOPS FP16\n",
    "    \"RTX2070\": 15e12,  # 15 TFLOPS FP16\n",
    "}\n",
    "\n",
    "# TODO Change these values to desired values\n",
    "batch_size = 20\n",
    "grad_accum = 5\n",
    "measured_time = 0.755  # in seconds per iteration\n",
    "\n",
    "# calculate flops achieved\n",
    "total_batch_size = batch_size * grad_accum\n",
    "measured_throughput = total_batch_size / measured_time\n",
    "flops_achieved = model_flops[\"total\"] * measured_throughput\n",
    "\n",
    "print(model_flops[\"total\"] // model_cfg.n_ctx)\n",
    "print(measured_throughput)\n",
    "\n",
    "# the fraction of the A100 that we are using:\n",
    "print(\"Fraction of GPU FLOPS used\")\n",
    "print(f\"{'GPU':14s} {'ratio (%)':10s}\")\n",
    "for k, v in GPU_FLOPS.items():\n",
    "    print(k, v)\n",
    "    print(f\"{k:14s} {flops_achieved / v * 100:10.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Training Compute\n",
    "\n",
    "Here we use the value computed so far to compute an estimate of the total amount of compute needed to train the model.\n",
    "\n",
    "Here we estimate the total amount of compute `C` required to train the model as `C ~= 6*N*D`, where:\n",
    "\n",
    "- `6` is a heuristic value (see [Dzmitry Bahdanau's post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) for an explanation) \n",
    "    - it basically stems from weight multiplications requiring 6 FLOPS per token per weight when combining both forward and backward passes.\n",
    "- `N` is the total number of model parameters\n",
    "- `D` is total size of dataset (in tokens)\n",
    "\n",
    "Note the equation changes to `C ~= 8*N*D` with activation checkpointing (i.e. recompute activations as needed to save memory when doing back-prop).\n",
    "\n",
    "We also need to factor in the model flops utilization (MFU) to correct for the fact that we cannot use 100% of the GPUs FLOPS due to memory bottlenecks, etc (again see Dzmitry's blog for examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "model_size = get_params(model_cfg)[\"total\"]  # this is number of parameters, N\n",
    "\n",
    "# TODO change these parameters\n",
    "tokens_num = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "num_gpus = 8  # number of GPUS used in parallel\n",
    "\n",
    "print(\"Time needed to train model on different GPUS\")\n",
    "print(f\"{'GPU':10s} {'time (days)':16s}\")\n",
    "for gpu_name, gpu_flops in GPU_FLOPS.items():\n",
    "    flops_throughput = gpu_flops * num_gpus * assumed_mfu\n",
    "    flops_needed = 6 * model_size * tokens_num  # 6ND\n",
    "    time_needed_s = flops_needed / flops_throughput  # in seconds\n",
    "    print(f\"{gpu_name:10s} {time_needed_s / 3600 / 24:10.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
