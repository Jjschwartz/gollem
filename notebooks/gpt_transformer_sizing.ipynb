{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Sizing\n",
    "\n",
    "This notebook runs a bunch of analysis about a GPT-2 Transformer, e.g. number of FLOPS, parameters, peak memory footprint, checkpoint size, etc\n",
    "\n",
    "**Reference**\n",
    "- This notebook is based directly on Karpathy's [nanoGPT/transformer_sizing.ipynb](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str\n",
    "    n_ctx: int = 1024\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    d_model: int = 768\n",
    "    d_mlp: int = 4 * 768\n",
    "    vocab_size: int = 50257\n",
    "    ln_bias: bool = False\n",
    "    mlp_bias: bool = False\n",
    "    share_embd_params: bool = True\n",
    "\n",
    "\n",
    "MODEL_CONFIG_ARGS = {\n",
    "    # 14M params\n",
    "    \"gpt2-tiny\": ModelConfig(\n",
    "        name=\"gpt2-tiny\",\n",
    "        n_ctx=128,\n",
    "        n_layer=2,\n",
    "        n_head=4,\n",
    "        d_model=256,\n",
    "        d_mlp=4 * 256,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 124M params\n",
    "    \"gpt2\": ModelConfig(\n",
    "        name=\"gpt2\",\n",
    "        n_ctx=1024,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        d_model=768,\n",
    "        d_mlp=4 * 768,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 350M params\n",
    "    \"gpt2-medium\": ModelConfig(\n",
    "        name=\"gpt2-medium\",\n",
    "        n_ctx=1024,\n",
    "        n_layer=24,\n",
    "        n_head=16,\n",
    "        d_model=1024,\n",
    "        d_mlp=4 * 1024,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 774M params\n",
    "    \"gpt2-large\": ModelConfig(\n",
    "        name=\"gpt2-large\",\n",
    "        n_ctx=1024,\n",
    "        n_layer=36,\n",
    "        n_head=20,\n",
    "        d_model=1280,\n",
    "        d_mlp=4 * 1280,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 1558M params\n",
    "    \"gpt2-xl\": ModelConfig(\n",
    "        name=\"gpt2-xl\",\n",
    "        n_ctx=1024,\n",
    "        n_layer=48,\n",
    "        n_head=25,\n",
    "        d_model=1600,\n",
    "        d_mlp=4 * 1600,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def load_config(name: str) -> ModelConfig:\n",
    "    assert name in MODEL_CONFIG_ARGS\n",
    "    return MODEL_CONFIG_ARGS[name]\n",
    "\n",
    "\n",
    "# TODO Change this as desired\n",
    "model_cfg = load_config(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 params     ratio (%) \n",
      "embedding/position      1638400     0.1052\n",
      "embedding/token        80411200     5.1635\n",
      "embedding              82049600     5.2687\n",
      "attention/ln               3200     0.0002\n",
      "attention/kqv           7680000     0.4932\n",
      "attention/proj          2560000     0.1644\n",
      "attention              10243200     0.6578\n",
      "mlp/ln                     3200     0.0002\n",
      "mlp/ffw                10246400     0.6580\n",
      "mlp/proj               10241600     0.6576\n",
      "mlp                    20491200     1.3158\n",
      "block                  30734400     1.9736\n",
      "transformer          1475251200    94.7311\n",
      "ln_f                       3200     0.0002\n",
      "out_embedding                 0     0.0000\n",
      "total                1557304000   100.0000\n"
     ]
    }
   ],
   "source": [
    "def get_params(cfg: ModelConfig):\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = cfg.n_ctx * cfg.d_model\n",
    "    out[\"embedding/token\"] = cfg.vocab_size * cfg.d_model\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "    out[\"attention/kqv\"] = cfg.d_model * 3 * cfg.d_model\n",
    "    out[\"attention/proj\"] = cfg.d_model**2\n",
    "    out[\"attention\"] = (\n",
    "        out[\"attention/ln\"] + out[\"attention/kqv\"] + out[\"attention/proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/ln\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "    out[\"mlp/ffw\"] = cfg.d_model * cfg.d_mlp + int(cfg.ln_bias) * cfg.d_mlp\n",
    "    out[\"mlp/proj\"] = cfg.d_mlp * cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "    out[\"mlp\"] = out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/proj\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "    out[\"ln_f\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model  # final layernorm\n",
    "    if cfg.share_embd_params:\n",
    "        # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "        out[\"out_embedding\"] = 0\n",
    "    else:\n",
    "        out[\"out_embedding\"] = cfg.d_model * cfg.vocab_size\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = (\n",
    "        out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"out_embedding\"]\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# compare our param count to that reported by PyTorch (for \"GPT2\" with 124M params)\n",
    "# TODO update the PyTorch value to include bias\n",
    "model_params = get_params(model_cfg)\n",
    "params_total = model_params[\"total\"]\n",
    "if model_cfg.name == \"gpt2\":\n",
    "    print(\n",
    "        f\"we see: {params_total}, expected: {124402944}, match: {params_total == 124337664}\"\n",
    "    )\n",
    "# create a header\n",
    "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "for k, v in model_params.items():\n",
    "    print(f\"{k:20s} {v:10d} {v / params_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter/Checkpoint Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est checkpoint size: 18.69 GB\n",
      "measured with wc -c ckpt.pt: 1542470366\n",
      "fluff ratio: 8.25%\n"
     ]
    }
   ],
   "source": [
    "# we can now calculate the size of each checkpoint\n",
    "# params are stored in fp32 (i.e. 4 bytes), and the AdamW optimizer has 2 additional buffers per param for statistics\n",
    "params_bytes = params_total * 4\n",
    "params_and_buffers_bytes = params_bytes + 2 * params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes / 1e9:.2f} GB\")\n",
    "# TODO update this with actual measured bytes\n",
    "measured_bytes = 1542470366  # from wc -c ckpt.pt\n",
    "print(f\"measured with wc -c ckpt.pt: {measured_bytes}\")\n",
    "print(f\"fluff ratio: {measured_bytes / params_and_buffers_bytes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU memory usage\n",
    "\n",
    "We can estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory ratio taken up just for parameters (incl. optimizer)\n",
      "GPU          ratio (%)\n",
      "H100            21.76\n",
      "A100            43.51\n",
      "RTX3090         72.52\n",
      "RTX4090         72.52\n",
      "RTX2070        217.55\n"
     ]
    }
   ],
   "source": [
    "# Nvidia reports memory in GiB (denoted as GB but is actually GiB)\n",
    "# 1 GiB = 1024 MiB = 1024**2 KiB = 1024**3 Bytes\n",
    "GPU_MEMORY = {\n",
    "    \"H100\": 80 * 1024**3,  # 80 GiB\n",
    "    \"A100\": 40 * 1024**3,  # 40 GiB\n",
    "    \"RTX3090\": 24 * 1024**3,  # 24 GiB\n",
    "    \"RTX4090\": 24 * 1024**3,  # 24 GiB\n",
    "    \"RTX2070\": 8 * 1024**3,  # 8 GiB\n",
    "}\n",
    "\n",
    "print(\"GPU memory ratio taken up just for parameters (incl. optimizer)\")\n",
    "print(f\"{'GPU':12s} {'ratio (%)':8s}\")\n",
    "for k, v in GPU_MEMORY.items():\n",
    "    print(f\"{k:12s} {params_and_buffers_bytes / v * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU forward-backward memory usage\n",
    "\n",
    "We can estimate the total memory usage of the model over the course of a forward-backward pass.\n",
    "\n",
    "This is made up of:\n",
    "\n",
    "1. M_model - the model and optimizer parameters\n",
    "2. M_optimizer - the optimizer buffers\n",
    "3. M_gradient - the gradient of the model\n",
    "4. M_activations - the activations of the model\n",
    "\n",
    "The activations scale with batch size and are typically the largest component of memory usage, since the others are fixed given model size and context length.\n",
    "\n",
    "Calculating the activation memory usage is quite tricky since it is affected by low level optimizations that can be hard to estimate exactly. It is also affected by things like flash attention, dropout, activation checkpointing, etc.\n",
    "\n",
    "Another source of memory usage is the CUDA and pytorch overhead which is typically 0.5-2GiB and will depend on the setup of the system that is running. We don't include this in the calculations, so factor this in when comparing calculated vs empirical memory usage.\n",
    "\n",
    "NOTE: these are estimates, so really should be taken as ballpark figures rather than exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_activations(cfg: ModelConfig, B: int, dtype: str, using_flash_attn: bool):\n",
    "    # Total: $8B + 16T + L \\times (34TBH + 5AT^2B) + 4TBH$\n",
    "    out = OrderedDict()\n",
    "\n",
    "    bytes_per_activation = 2 if dtype in [\"bfloat16\", \"float16\"] else 4\n",
    "    bytes_per_long = 8\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = cfg.n_ctx * bytes_per_long\n",
    "    out[\"embedding/token\"] = cfg.n_ctx * B * bytes_per_long\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    TBH = cfg.n_ctx * B * cfg.d_model\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = TBH * bytes_per_activation\n",
    "    out[\"attention/kqv\"] = TBH * bytes_per_activation\n",
    "    if using_flash_attn:\n",
    "        # when using flash attention a bunch of optimizations are done\n",
    "        out[\"attention/qk_matmul\"] = 0\n",
    "        out[\"attention/softmax\"] = 0\n",
    "        # flash attention requires K, Q, V as well as two vectors l, m of length T (size BT)\n",
    "        out[\"attention/attention_over_v\"] = (\n",
    "            TBH * 3 + 2 * cfg.n_ctx * B\n",
    "        ) * bytes_per_activation\n",
    "    else:\n",
    "        out[\"attention/qk_matmul\"] = TBH * 2 * bytes_per_activation\n",
    "        out[\"attention/softmax\"] = cfg.n_head * cfg.n_ctx**2 * B * bytes_per_activation\n",
    "        out[\"attention/attention_over_v\"] = (\n",
    "            TBH + cfg.n_head * cfg.n_ctx**2 * B\n",
    "        ) * bytes_per_activation\n",
    "    out[\"attention/proj\"] = TBH * bytes_per_activation\n",
    "    out[\"attention\"] = (\n",
    "        out[\"attention/ln\"]\n",
    "        + out[\"attention/kqv\"]\n",
    "        + out[\"attention/qk_matmul\"]\n",
    "        + out[\"attention/softmax\"]\n",
    "        + out[\"attention/attention_over_v\"]\n",
    "        + out[\"attention/proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/ln\"] = TBH * bytes_per_activation\n",
    "    out[\"mlp/ffw\"] = TBH * bytes_per_activation\n",
    "    out[\"mlp/ffw_activation\"] = cfg.n_ctx * B * cfg.d_mlp * bytes_per_activation\n",
    "    out[\"mlp/proj\"] = cfg.n_ctx * B * cfg.d_mlp * bytes_per_activation\n",
    "    out[\"mlp\"] = (\n",
    "        out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/ffw_activation\"] + out[\"mlp/proj\"]\n",
    "    )\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "\n",
    "    # final layernorm and output projection\n",
    "    out[\"ln_f\"] = TBH * bytes_per_activation\n",
    "    out[\"out_embedding\"] = TBH * bytes_per_activation\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = (\n",
    "        out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"out_embedding\"]\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_model: 8.70 GiB\n",
      "M_optimizer: 11.60 GiB\n",
      "M_gradient: 5.80 GiB\n",
      "batch size M_activations total memory   diff\n",
      "         1          3.67        29.77 -27.56\n",
      "         2          7.34        33.44 -30.64\n",
      "         4         14.67        40.78 -36.95\n",
      "         8         29.35        55.45 -49.58\n",
      "        16         58.69        84.80 -74.83\n",
      "        32        117.39       143.50 -125.34\n"
     ]
    }
   ],
   "source": [
    "dtype = \"bfloat16\"\n",
    "using_flash_attn = True\n",
    "# dtype = \"float32\"\n",
    "\n",
    "# when using bf16 or fp16 we use mixed precision so have tostore\n",
    "# both fp16 and fp32 versions of the parameters\n",
    "# when using fp32 we store only the full precision fp32 parameters\n",
    "bytes_per_param = 6 if dtype in (\"bfloat16\", \"float16\") else 4\n",
    "M_model = params_total * bytes_per_param\n",
    "\n",
    "# AdamW optimizer has 2 buffers per parameter\n",
    "# values are stored in fp32\n",
    "M_optimizer = 2 * params_total * 4\n",
    "\n",
    "# we store one gradient value per parameter\n",
    "# Gradient are stored in fp32\n",
    "M_gradient = params_total * 4\n",
    "\n",
    "\n",
    "divisor = 1024**3\n",
    "\n",
    "print(f\"M_model: {M_model / divisor:.2f} GiB\")\n",
    "print(f\"M_optimizer: {M_optimizer / divisor:.2f} GiB\")\n",
    "print(f\"M_gradient: {M_gradient / divisor:.2f} GiB\")\n",
    "\n",
    "\n",
    "# Empirical peak memory usage for gpt2 (124M) using flash attention\n",
    "# run_name\t            \t peak_mem_usage\n",
    "# batch_size=1024 (1x1024)\t 2269\n",
    "# batch_size=2048 (2x1024)\t 2870\n",
    "# batch_size=4096 (4x1024)\t 3918\n",
    "# batch_size=8192 (8x1024)\t 6012\n",
    "# batch_size=16384 (16x1024) 10206\n",
    "# batch_size=32768 (32x1024) 18595\n",
    "# Map from batch size to peak memory usage in MiB\n",
    "empirical_peak_memory_usage = {\n",
    "    1: 2269,\n",
    "    2: 2870,\n",
    "    4: 3918,\n",
    "    8: 6012,\n",
    "    16: 10206,\n",
    "    32: 18595,\n",
    "}\n",
    "\n",
    "# M_activations is more complex\n",
    "print(f\"{'batch size':10s} {'M_activations':13s} {'total memory':12s} {'  diff':6s}\")\n",
    "for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "    M_activations = compute_activations(model_cfg, batch_size, dtype, using_flash_attn)\n",
    "    total_memory = M_model + M_optimizer + M_activations[\"total\"] + M_gradient\n",
    "    total_memory_GiB = total_memory / divisor\n",
    "    diff = (empirical_peak_memory_usage[batch_size] / 1024) - total_memory_GiB\n",
    "    print(\n",
    "        f\"{batch_size:10d} {M_activations['total'] / divisor:13.2f} {total_memory_GiB:12.2f} {diff:6.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOPS\n",
    "\n",
    "Here we estimate FLOPS for a single forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 flops          ratio (%) \n",
      "attention/kqv           15728640000     0.4485\n",
      "attention/scores         3355443200     0.0957\n",
      "attention/reduce         3355443200     0.0957\n",
      "attention/proj           5242880000     0.1495\n",
      "attention               27682406400     0.7894\n",
      "mlp/ffw1                20971520000     0.5980\n",
      "mlp/ffw2                20971520000     0.5980\n",
      "mlp                     41943040000     1.1961\n",
      "block                   69625446400     1.9855\n",
      "transformer           3342021427200    95.3038\n",
      "out_embedding          164682137600     4.6962\n",
      "forward_total         3506703564800   100.0000\n",
      "backward_total        7013407129600   200.0000\n",
      "total                10520110694400   300.0000\n"
     ]
    }
   ],
   "source": [
    "def compute_flops(cfg: ModelConfig):\n",
    "    # we only count Weight FLOPs,\n",
    "    # FLOPS for all other layers (LayerNorm, Softmax, etc) and bias vector additian are effectively irrelevant\n",
    "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
    "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    head_size = cfg.d_model // cfg.n_head\n",
    "\n",
    "    # attention blocks\n",
    "    # 1) the projection to key, query, values\n",
    "    out[\"attention/kqv\"] = 2 * cfg.n_ctx * (cfg.d_model * 3 * cfg.d_model)\n",
    "    # 2) calculating the attention scores\n",
    "    out[\"attention/scores\"] = 2 * cfg.n_ctx * cfg.n_ctx * cfg.d_model\n",
    "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out[\"attention/reduce\"] = 2 * cfg.n_head * (cfg.n_ctx * cfg.n_ctx * head_size)\n",
    "    # 4) the final linear projection\n",
    "    out[\"attention/proj\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.d_model)\n",
    "    out[\"attention\"] = sum(\n",
    "        out[\"attention/\" + k] for k in [\"kqv\", \"scores\", \"reduce\", \"proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/ffw1\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.d_mlp)\n",
    "    out[\"mlp/ffw2\"] = 2 * cfg.n_ctx * (cfg.d_mlp * cfg.d_model)\n",
    "    out[\"mlp\"] = out[\"mlp/ffw1\"] + out[\"mlp/ffw2\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "    out[\"out_embedding\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.vocab_size)\n",
    "\n",
    "    # forward,backward,total\n",
    "    out[\"forward_total\"] = out[\"transformer\"] + out[\"out_embedding\"]\n",
    "    out[\"backward_total\"] = (\n",
    "        2 * out[\"forward_total\"]\n",
    "    )  # use common estimate of bwd = 2*fwd\n",
    "    out[\"total\"] = out[\"forward_total\"] + out[\"backward_total\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# compare our param count to that reported by PyTorch\n",
    "model_flops = compute_flops(model_cfg)\n",
    "flops_total = model_flops[\"forward_total\"]\n",
    "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
    "for k, v in model_flops.items():\n",
    "    print(f\"{k:20s} {v:14d} {v / flops_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palm_flops: 10524377088000, flops: 10520110694400, ratio: 1.0004\n"
     ]
    }
   ],
   "source": [
    "# now here is an estimate copy pasted from the PaLM paper\n",
    "# this formula is often used to calculate MFU (model flops utilization)\n",
    "def compute_palm_flops(cfg: ModelConfig):\n",
    "    \"\"\"estimate of the model flops following PaLM paper formula\"\"\"\n",
    "    # non-embedding model parameters. note that we do not subtract the\n",
    "    # embedding/token params because those are tied and get used in the last layer.\n",
    "    model_params = get_params(cfg)\n",
    "    N = model_params[\"total\"] - model_params[\"embedding/position\"]\n",
    "    L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.d_model // cfg.n_head, cfg.n_ctx\n",
    "    mf_per_token = 6 * N + 12 * L * H * Q * T\n",
    "    mf = mf_per_token * cfg.n_ctx\n",
    "    return mf\n",
    "\n",
    "\n",
    "palm_flops = compute_palm_flops(model_cfg)\n",
    "print(\n",
    "    f\"palm_flops: {palm_flops:d}, flops: {model_flops['total']:d}, ratio: {palm_flops / model_flops['total']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Flops Usage\n",
    "\n",
    "Given our estimated FLOPS we can calculate how much of our GPU FLOP capacity is being used, that is our model flop utilization (MFU).\n",
    "\n",
    "To calculate this we need a few bits of information:\n",
    "\n",
    "- GPU speed (FLOPS)\n",
    "- batch size (including gradient accumulation)\n",
    "- time per iteration\n",
    "\n",
    "For the GPU speed we refer to: https://www.techpowerup.com/gpu-specs/ which gives theoretical performance, specifically looking at performance for BF16 and FP16, which ever is supported and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of GPU FLOPS used\n",
      "GPU            ratio (%) \n",
      "H100               184.31\n",
      "A100               446.60\n",
      "RTX4090           1678.79\n",
      "RTX2070           9289.28\n"
     ]
    }
   ],
   "source": [
    "# R\n",
    "GPU_FLOPS = {\n",
    "    \"H100\": 756e12,  # 756 TFLOPS BF16 (this is a guess, spec sheet shows 1513 for sparse tensors)\n",
    "    \"A100\": 312e12,  # 312 TFLOPS BF16\n",
    "    \"RTX4090\": 83e12,  # 83 TFLOPS FP16\n",
    "    \"RTX2070\": 15e12,  # 15 TFLOPS FP16\n",
    "}\n",
    "\n",
    "# TODO Change these values to desired values\n",
    "batch_size = 20\n",
    "grad_accum = 5\n",
    "measured_time = 0.755  # in seconds per iteration\n",
    "\n",
    "# calculate flops achieved\n",
    "total_batch_size = batch_size * grad_accum\n",
    "measured_throughput = total_batch_size / measured_time\n",
    "flops_achieved = model_flops[\"total\"] * measured_throughput\n",
    "\n",
    "# the fraction of the A100 that we are using:\n",
    "print(\"Fraction of GPU FLOPS used\")\n",
    "print(f\"{'GPU':14s} {'ratio (%)':10s}\")\n",
    "for k, v in GPU_FLOPS.items():\n",
    "    print(f\"{k:14s} {flops_achieved / v * 100:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Training Compute\n",
    "\n",
    "Here we use the value computed so far to compute an estimate of the total amount of compute needed to train the model.\n",
    "\n",
    "Here we estimate the total amount of compute `C` required to train the model as `C ~= 6*N*D`, where:\n",
    "\n",
    "- `6` is a heuristic value (see [Dzmitry Bahdanau's post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) for an explanation) \n",
    "    - it basically stems from weight multiplications requiring 6 FLOPS per token per weight when combining both forward and backward passes.\n",
    "- `N` is the total number of model parameters\n",
    "- `D` is total size of dataset (in tokens)\n",
    "\n",
    "Note the equation changes to `C ~= 8*N*D` with activation checkpointing (i.e. recompute activations as needed to save memory when doing back-prop).\n",
    "\n",
    "We also need to factor in the model flops utilization (MFU) to correct for the fact that we cannot use 100% of the GPUs FLOPS due to memory bottlenecks, etc (again see Dzmitry's blog for examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to train model on different GPUS\n",
      "GPU        time (days)     \n",
      "H100            17.88\n",
      "A100            43.33\n",
      "RTX4090        162.87\n",
      "RTX2070        901.22\n"
     ]
    }
   ],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "model_size = get_params(model_cfg)[\"total\"]  # this is number of parameters, N\n",
    "\n",
    "# TODO change these parameters\n",
    "tokens_num = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "num_gpus = 8  # number of GPUS used in parallel\n",
    "\n",
    "print(\"Time needed to train model on different GPUS\")\n",
    "print(f\"{'GPU':10s} {'time (days)':16s}\")\n",
    "for gpu_name, gpu_flops in GPU_FLOPS.items():\n",
    "    flops_throughput = gpu_flops * num_gpus * assumed_mfu\n",
    "    flops_needed = 6 * model_size * tokens_num  # 6ND\n",
    "    time_needed_s = flops_needed / flops_throughput  # in seconds\n",
    "    print(f\"{gpu_name:10s} {time_needed_s / 3600 / 24:10.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
