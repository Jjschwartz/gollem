{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation checkpointing\n",
    "\n",
    "Notebook exploring how much memory activation checkpointing saves under different setups, e.g. different model sizes, different sequence lengths, checkpointing different layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_ctx: int = 1024\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    d_model: int = 768\n",
    "    d_mlp: int = 4 * 768\n",
    "    vocab_size: int = 50257\n",
    "    ln_bias: bool = False\n",
    "    mlp_bias: bool = False\n",
    "    share_embd_params: bool = True\n",
    "\n",
    "\n",
    "MODEL_CONFIG_ARGS = {\n",
    "    # 14M params\n",
    "    \"gpt2-tiny\": ModelConfig(\n",
    "        n_ctx=128,\n",
    "        n_layer=2,\n",
    "        n_head=4,\n",
    "        d_model=256,\n",
    "        d_mlp=4 * 256,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 124M params\n",
    "    \"gpt2\": ModelConfig(\n",
    "        n_ctx=1024,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        d_model=768,\n",
    "        d_mlp=4 * 768,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 350M params\n",
    "    \"gpt2-medium\": ModelConfig(\n",
    "        n_ctx=1024,\n",
    "        n_layer=24,\n",
    "        n_head=16,\n",
    "        d_model=1024,\n",
    "        d_mlp=4 * 1024,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 774M params\n",
    "    \"gpt2-large\": ModelConfig(\n",
    "        n_ctx=1024,\n",
    "        n_layer=36,\n",
    "        n_head=20,\n",
    "        d_model=1280,\n",
    "        d_mlp=4 * 1280,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "    # 1558M params\n",
    "    \"gpt2-xl\": ModelConfig(\n",
    "        n_ctx=1024,\n",
    "        n_layer=48,\n",
    "        n_head=25,\n",
    "        d_model=1600,\n",
    "        d_mlp=4 * 1600,\n",
    "        vocab_size=50257,\n",
    "        ln_bias=True,\n",
    "        mlp_bias=True,\n",
    "        share_embd_params=True,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def load_config(name: str) -> ModelConfig:\n",
    "    assert name in MODEL_CONFIG_ARGS\n",
    "    return MODEL_CONFIG_ARGS[name]\n",
    "\n",
    "\n",
    "# TODO Change this as desired\n",
    "model_cfg = load_config(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(cfg: ModelConfig, checkpoint_layers: set[str] | None = None):\n",
    "    \"\"\"Estimates the number of parameters in the model.\"\"\"\n",
    "    out = OrderedDict()\n",
    "    if checkpoint_layers is None:\n",
    "        checkpoint_layers = set()\n",
    "\n",
    "    # token and position embeddings\n",
    "    if \"embedding\" not in checkpoint_layers:\n",
    "        out[\"embedding/position\"] = cfg.n_ctx * cfg.d_model\n",
    "        out[\"embedding/token\"] = cfg.vocab_size * cfg.d_model\n",
    "        out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "    else:\n",
    "        out[\"embedding\"] = 0\n",
    "\n",
    "    # attention blocks\n",
    "    if \"attention\" not in checkpoint_layers:\n",
    "        out[\"attention/ln\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "        out[\"attention/kqv\"] = cfg.d_model * 3 * cfg.d_model\n",
    "        out[\"attention/proj\"] = cfg.d_model**2\n",
    "        out[\"attention\"] = (\n",
    "            out[\"attention/ln\"] + out[\"attention/kqv\"] + out[\"attention/proj\"]\n",
    "        )\n",
    "    else:\n",
    "        out[\"attention\"] = 0\n",
    "\n",
    "    # MLP blocks\n",
    "    if \"mlp\" not in checkpoint_layers:\n",
    "        out[\"mlp/ln\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "        out[\"mlp/ffw\"] = cfg.d_model * cfg.d_mlp + int(cfg.ln_bias) * cfg.d_mlp\n",
    "        out[\"mlp/proj\"] = cfg.d_mlp * cfg.d_model + int(cfg.ln_bias) * cfg.d_model\n",
    "        out[\"mlp\"] = out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/proj\"]\n",
    "    else:\n",
    "        out[\"mlp\"] = 0\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "    out[\"ln_f\"] = cfg.d_model + int(cfg.ln_bias) * cfg.d_model  # final layernorm\n",
    "    if cfg.share_embd_params:\n",
    "        # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "        out[\"out_embedding\"] = 0\n",
    "    else:\n",
    "        out[\"out_embedding\"] = cfg.d_model * cfg.vocab_size\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = (\n",
    "        out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"out_embedding\"]\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                             Activation Checkpointing                                              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Checkpoint       </span>┃<span style=\"font-weight: bold\"> Forward Params    </span>┃<span style=\"font-weight: bold\"> Total Params     </span>┃<span style=\"font-weight: bold\">                   </span>┃<span style=\"font-weight: bold\">                </span>┃<span style=\"font-weight: bold\"> Memory Savings   </span>┃\n",
       "┃<span style=\"font-weight: bold\"> Policy           </span>┃<span style=\"font-weight: bold\"> Stored            </span>┃<span style=\"font-weight: bold\"> Stored           </span>┃<span style=\"font-weight: bold\"> Total Memory (GB) </span>┃<span style=\"font-weight: bold\"> Params Savings </span>┃<span style=\"font-weight: bold\"> (%)              </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ None             │ 124402944         │ 373208832        │ 0.70              │ 66.67%         │ 50.00%           │\n",
       "│ ('embedding',)   │ 85019136          │ 333825024        │ 0.62              │ 77.22%         │ 44.10%           │\n",
       "│ ('attention',)   │ 96072960          │ 344878848        │ 0.64              │ 74.26%         │ 45.89%           │\n",
       "│ ('mlp',)         │ 67715328          │ 316521216        │ 0.59              │ 81.86%         │ 41.05%           │\n",
       "│ ('embedding',    │ 56689152          │ 305495040        │ 0.57              │ 84.81%         │ 38.92%           │\n",
       "│ 'attention')     │                   │                  │                   │                │                  │\n",
       "│ ('embedding',    │ 28331520          │ 277137408        │ 0.52              │ 92.41%         │ 32.67%           │\n",
       "│ 'mlp')           │                   │                  │                   │                │                  │\n",
       "│ ('attention',    │ 39385344          │ 288191232        │ 0.54              │ 89.45%         │ 35.25%           │\n",
       "│ 'mlp')           │                   │                  │                   │                │                  │\n",
       "│ ('embedding',    │ 1536              │ 248807424        │ 0.46              │ 100.00%        │ 25.00%           │\n",
       "│ 'attention',     │                   │                  │                   │                │                  │\n",
       "│ 'mlp')           │                   │                  │                   │                │                  │\n",
       "└──────────────────┴───────────────────┴──────────────────┴───────────────────┴────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                             Activation Checkpointing                                              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCheckpoint      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mForward Params   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTotal Params    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                   \u001b[0m┃\u001b[1m                \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mMemory Savings  \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPolicy          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStored           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStored          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTotal Memory (GB)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mParams Savings\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(%)             \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ None             │ 124402944         │ 373208832        │ 0.70              │ 66.67%         │ 50.00%           │\n",
       "│ ('embedding',)   │ 85019136          │ 333825024        │ 0.62              │ 77.22%         │ 44.10%           │\n",
       "│ ('attention',)   │ 96072960          │ 344878848        │ 0.64              │ 74.26%         │ 45.89%           │\n",
       "│ ('mlp',)         │ 67715328          │ 316521216        │ 0.59              │ 81.86%         │ 41.05%           │\n",
       "│ ('embedding',    │ 56689152          │ 305495040        │ 0.57              │ 84.81%         │ 38.92%           │\n",
       "│ 'attention')     │                   │                  │                   │                │                  │\n",
       "│ ('embedding',    │ 28331520          │ 277137408        │ 0.52              │ 92.41%         │ 32.67%           │\n",
       "│ 'mlp')           │                   │                  │                   │                │                  │\n",
       "│ ('attention',    │ 39385344          │ 288191232        │ 0.54              │ 89.45%         │ 35.25%           │\n",
       "│ 'mlp')           │                   │                  │                   │                │                  │\n",
       "│ ('embedding',    │ 1536              │ 248807424        │ 0.46              │ 100.00%        │ 25.00%           │\n",
       "│ 'attention',     │                   │                  │                   │                │                  │\n",
       "│ 'mlp')           │                   │                  │                   │                │                  │\n",
       "└──────────────────┴───────────────────┴──────────────────┴───────────────────┴────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "possible_checkpoints = [\"embedding\", \"attention\", \"mlp\"]\n",
    "\n",
    "base_params = get_params(model_cfg)[\"total\"]\n",
    "n_ctx = model_cfg.n_ctx\n",
    "\n",
    "params_by_checkpoint_policy = {\n",
    "    \"None\": base_params,\n",
    "}\n",
    "# iterate through all possible permutation of checkpointing,\n",
    "# e.g. [[\"embedding\"], [\"embedding\", \"attention\"], [\"embedding\", \"attention\", \"mlp\"]]\n",
    "for checkpoint_combo in itertools.chain.from_iterable(\n",
    "    itertools.combinations(possible_checkpoints, i)\n",
    "    for i in range(1, len(possible_checkpoints) + 1)\n",
    "):\n",
    "    params = get_params(model_cfg, set(checkpoint_combo))[\"total\"]\n",
    "    params_by_checkpoint_policy[str(checkpoint_combo)] = params\n",
    "\n",
    "\n",
    "total_base_params = base_params + (base_params * n_ctx * 2)\n",
    "base_memory_gb = total_base_params / 1024**3\n",
    "\n",
    "table_headers = [\n",
    "    \"Checkpoint Policy\",\n",
    "    \"Forward Params Stored\",\n",
    "    \"Total Params Stored\",\n",
    "    \"Total Memory (GB)\",\n",
    "    \"Params Savings\",\n",
    "    \"Memory Savings (%)\",\n",
    "]\n",
    "table_rows = []\n",
    "for checkpoint_policy, forward_params in params_by_checkpoint_policy.items():\n",
    "    # Assume we are using 16-bit precision (i.e. 2 bytes per parameter)\n",
    "    # Checkpointing still stores the full model + backwards activations, just\n",
    "    # not some forward activations.\n",
    "    n_forward_activations = forward_params * n_ctx\n",
    "    n_backward_activations = base_params * n_ctx\n",
    "    total_params_stored = base_params + ((base_params + forward_params) * n_ctx * 2)\n",
    "    total_memory_bytes = total_params_stored * 2\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "    params_savings = (total_base_params - forward_params) / total_base_params * 100\n",
    "    memory_savings = (total_memory_gb - base_memory_gb) / total_memory_gb * 100\n",
    "    table_rows.append(\n",
    "        [\n",
    "            checkpoint_policy,\n",
    "            str(forward_params),\n",
    "            str(total_params_stored),\n",
    "            f\"{total_memory_gb:.2f}\",\n",
    "            f\"{params_savings:.2f}%\",\n",
    "            f\"{memory_savings:.2f}%\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "table = Table(title=\"Activation Checkpointing\")\n",
    "for header in table_headers:\n",
    "    table.add_column(header)\n",
    "for row in table_rows:\n",
    "    table.add_row(*row)\n",
    "\n",
    "console = Console()\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
