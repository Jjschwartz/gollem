{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Sizing\n",
    "\n",
    "This notebook runs a bunch of analysis about a GPT-2 Transformer, e.g. number of FLOPS, parameters, peak memory footprint, checkpoint size, etc\n",
    "\n",
    "**Reference**\n",
    "- This notebook is based directly on Karpathy's [nanoGPT/transformer_sizing.ipynb](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/gollem/gollem/models/llama3/model.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from gollem.models.llama3.config import Llama3Config\n",
    "from gollem.models.llama3.config import get_llama3_model_config\n",
    "\n",
    "\n",
    "# TODO Change this as desired\n",
    "model_name = \"llama3-1B\"\n",
    "\n",
    "model_cfg = get_llama3_model_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 params     ratio (%) \n",
      "embedding/token       262144000    18.3276\n",
      "embedding             262144000    18.3276\n",
      "attention/norm             2048     0.0001\n",
      "attention/wq            4194304     0.2932\n",
      "attention/wk            2097152     0.1466\n",
      "attention/wv            2097152     0.1466\n",
      "attention/wo            4194304     0.2932\n",
      "attention              12584960     0.8799\n",
      "attention_total       201359360    14.0779\n",
      "mlp/norm                   2048     0.0001\n",
      "mlp/w1                 14680064     1.0263\n",
      "mlp/w2                 14680064     1.0263\n",
      "mlp/w3                 14680064     1.0263\n",
      "mlp                    44042240     3.0792\n",
      "mlp_total             704675840    49.2668\n",
      "block                  56627200     3.9590\n",
      "transformer           906035200    63.3447\n",
      "norm_final                 2048     0.0001\n",
      "out_embedding         262144000    18.3276\n",
      "total                1430325248   100.0000\n"
     ]
    }
   ],
   "source": [
    "def get_params(cfg: Llama3Config):\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token embeddings\n",
    "    out[\"embedding/token\"] = cfg.vocab_size * cfg.d_model\n",
    "    out[\"embedding\"] = out[\"embedding/token\"]\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/norm\"] = cfg.d_model\n",
    "    d_head = cfg.d_model // cfg.n_head\n",
    "    out[\"attention/wq\"] = cfg.d_model * cfg.d_model\n",
    "    out[\"attention/wk\"] = cfg.d_model * cfg.n_kv_head * d_head\n",
    "    out[\"attention/wv\"] = cfg.d_model * cfg.n_kv_head * d_head\n",
    "    out[\"attention/wo\"] = cfg.d_model * cfg.d_model\n",
    "    out[\"attention\"] = (\n",
    "        out[\"attention/norm\"]\n",
    "        + out[\"attention/wq\"]\n",
    "        + out[\"attention/wk\"]\n",
    "        + out[\"attention/wv\"]\n",
    "        + out[\"attention/wo\"]\n",
    "    )\n",
    "    out[\"attention_total\"] = cfg.n_layer * out[\"attention\"]\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/norm\"] = cfg.d_model\n",
    "    out[\"mlp/w1\"] = cfg.d_model * cfg.intermediate_size\n",
    "    out[\"mlp/w2\"] = cfg.intermediate_size * cfg.d_model\n",
    "    out[\"mlp/w3\"] = cfg.d_model * cfg.intermediate_size\n",
    "    out[\"mlp\"] = out[\"mlp/norm\"] + out[\"mlp/w1\"] + out[\"mlp/w2\"] + out[\"mlp/w3\"]\n",
    "    out[\"mlp_total\"] = cfg.n_layer * out[\"mlp\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "    out[\"norm_final\"] = cfg.d_model\n",
    "    out[\"out_embedding\"] = cfg.d_model * cfg.vocab_size\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = (\n",
    "        out[\"embedding\"] + out[\"transformer\"] + out[\"norm_final\"] + out[\"out_embedding\"]\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "model_params = get_params(model_cfg)\n",
    "params_total = model_params[\"total\"]\n",
    "\n",
    "# create a header\n",
    "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "for k, v in model_params.items():\n",
    "    print(f\"{k:20s} {v:10d} {v / params_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter/Checkpoint Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est checkpoint size: 17.16 GB\n",
      "measured with wc -c ckpt.pt: 1542470366\n",
      "fluff ratio: 8.99%\n"
     ]
    }
   ],
   "source": [
    "# we can now calculate the size of each checkpoint\n",
    "# params are stored in fp32 (i.e. 4 bytes), and the AdamW optimizer has 2 additional buffers per param for statistics\n",
    "params_bytes = params_total * 4\n",
    "params_and_buffers_bytes = params_bytes + 2 * params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes / 1e9:.2f} GB\")\n",
    "# TODO update this with actual measured bytes\n",
    "measured_bytes = 1542470366  # from wc -c ckpt.pt\n",
    "print(f\"measured with wc -c ckpt.pt: {measured_bytes}\")\n",
    "print(f\"fluff ratio: {measured_bytes / params_and_buffers_bytes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU memory usage\n",
    "\n",
    "We can estimate the ratio of our GPU memory that will be taken up just by the weights and the buffers inside the AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory ratio taken up just for parameters (incl. optimizer)\n",
      "GPU          ratio (%)\n",
      "H100            19.98\n",
      "A100            39.96\n",
      "RTX3090         66.60\n",
      "RTX4090         66.60\n",
      "RTX2070        199.81\n"
     ]
    }
   ],
   "source": [
    "# Nvidia reports memory in GiB (denoted as GB but is actually GiB)\n",
    "# 1 GiB = 1024 MiB = 1024**2 KiB = 1024**3 Bytes\n",
    "GPU_MEMORY = {\n",
    "    \"H100\": 80 * 1024**3,  # 80 GiB\n",
    "    \"A100\": 40 * 1024**3,  # 40 GiB\n",
    "    \"RTX3090\": 24 * 1024**3,  # 24 GiB\n",
    "    \"RTX4090\": 24 * 1024**3,  # 24 GiB\n",
    "    \"RTX2070\": 8 * 1024**3,  # 8 GiB\n",
    "}\n",
    "\n",
    "print(\"GPU memory ratio taken up just for parameters (incl. optimizer)\")\n",
    "print(f\"{'GPU':12s} {'ratio (%)':8s}\")\n",
    "for k, v in GPU_MEMORY.items():\n",
    "    print(f\"{k:12s} {params_and_buffers_bytes / v * 100:8.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU forward-backward memory usage\n",
    "\n",
    "We can estimate the total memory usage of the model over the course of a forward-backward pass.\n",
    "\n",
    "This is made up of:\n",
    "\n",
    "1. M_model - the model and optimizer parameters\n",
    "2. M_optimizer - the optimizer buffers\n",
    "3. M_gradient - the gradient of the model\n",
    "4. M_activations - the activations of the model\n",
    "\n",
    "The activations scale with batch size and are typically the largest component of memory usage, since the others are fixed given model size and context length.\n",
    "\n",
    "Calculating the activation memory usage is quite tricky since it is affected by low level optimizations that can be hard to estimate exactly. It is also affected by things like flash attention, dropout, activation checkpointing, etc.\n",
    "\n",
    "Another source of memory usage is the CUDA and pytorch overhead which is typically 0.5-2GiB and will depend on the setup of the system that is running. We don't include this in the calculations, so factor this in when comparing calculated vs empirical memory usage.\n",
    "\n",
    "NOTE: these are estimates, so really should be taken as ballpark figures rather than exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_activations(cfg: Llama3Config, B: int, dtype: str):\n",
    "    # Total: $8B + 16T + L \\times (34TBH + 5AT^2B) + 4TBH$\n",
    "    out = OrderedDict()\n",
    "\n",
    "    bytes_per_activation = 2 if dtype in [\"bfloat16\", \"float16\"] else 4\n",
    "    bytes_per_long = 8\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding\"] = cfg.n_ctx * B * bytes_per_long\n",
    "\n",
    "    TBH = cfg.n_ctx * B * cfg.d_model\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/norm\"] = TBH * bytes_per_activation\n",
    "    out[\"attention/kqv\"] = TBH * bytes_per_activation\n",
    "\n",
    "    # flash attention requires K, Q, V as well as two vectors l, m of length T (size BT)\n",
    "    out[\"attention/attention_over_v\"] = (\n",
    "        TBH * 3 + 2 * cfg.n_ctx * B\n",
    "    ) * bytes_per_activation\n",
    "\n",
    "    out[\"attention/proj\"] = TBH * bytes_per_activation\n",
    "    out[\"attention\"] = (\n",
    "        out[\"attention/norm\"]\n",
    "        + out[\"attention/kqv\"]\n",
    "        + out[\"attention/attention_over_v\"]\n",
    "        + out[\"attention/proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/norm\"] = TBH * bytes_per_activation\n",
    "    out[\"mlp/w1\"] = TBH * bytes_per_activation\n",
    "    out[\"mlp/w3\"] = TBH * bytes_per_activation\n",
    "    out[\"mlp/silu\"] = cfg.n_ctx * B * cfg.intermediate_size * bytes_per_activation\n",
    "    out[\"mlp/w2\"] = cfg.n_ctx * B * cfg.intermediate_size * bytes_per_activation\n",
    "    out[\"mlp\"] = (\n",
    "        out[\"mlp/norm\"]\n",
    "        + out[\"mlp/w1\"]\n",
    "        + out[\"mlp/w2\"]\n",
    "        + out[\"mlp/w3\"]\n",
    "        + out[\"mlp/silu\"]\n",
    "    )\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "\n",
    "    # final layernorm and output projection\n",
    "    out[\"norm_final\"] = TBH * bytes_per_activation\n",
    "    out[\"out_embedding\"] = TBH * bytes_per_activation\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = (\n",
    "        out[\"embedding\"] + out[\"transformer\"] + out[\"norm_final\"] + out[\"out_embedding\"]\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_model: 7.99 GiB\n",
      "M_optimizer: 10.66 GiB\n",
      "M_gradient: 5.33 GiB\n",
      "batch size M_activations total memory   diff\n",
      "         1          1.01        24.99 -22.77\n",
      "         2          2.02        25.99 -23.19\n",
      "         4          4.03        28.01 -24.18\n",
      "         8          8.06        32.04 -26.17\n",
      "        16         16.13        40.10 -30.14\n",
      "        32         32.25        56.23 -38.07\n"
     ]
    }
   ],
   "source": [
    "dtype = \"bfloat16\"\n",
    "# dtype = \"float32\"\n",
    "\n",
    "# when using bf16 or fp16 we use mixed precision so have tostore\n",
    "# both fp16 and fp32 versions of the parameters\n",
    "# when using fp32 we store only the full precision fp32 parameters\n",
    "bytes_per_param = 6 if dtype in (\"bfloat16\", \"float16\") else 4\n",
    "M_model = params_total * bytes_per_param\n",
    "\n",
    "# AdamW optimizer has 2 buffers per parameter\n",
    "# values are stored in fp32\n",
    "M_optimizer = 2 * params_total * 4\n",
    "\n",
    "# we store one gradient value per parameter\n",
    "# Gradient are stored in fp32\n",
    "M_gradient = params_total * 4\n",
    "\n",
    "\n",
    "divisor = 1024**3\n",
    "\n",
    "print(f\"M_model: {M_model / divisor:.2f} GiB\")\n",
    "print(f\"M_optimizer: {M_optimizer / divisor:.2f} GiB\")\n",
    "print(f\"M_gradient: {M_gradient / divisor:.2f} GiB\")\n",
    "\n",
    "\n",
    "# Empirical peak memory usage for gpt2 (124M) using flash attention\n",
    "# run_name\t            \t peak_mem_usage\n",
    "# batch_size=1024 (1x1024)\t 2269\n",
    "# batch_size=2048 (2x1024)\t 2870\n",
    "# batch_size=4096 (4x1024)\t 3918\n",
    "# batch_size=8192 (8x1024)\t 6012\n",
    "# batch_size=16384 (16x1024) 10206\n",
    "# batch_size=32768 (32x1024) 18595\n",
    "# Map from batch size to peak memory usage in MiB\n",
    "empirical_peak_memory_usage = {\n",
    "    1: 2269,\n",
    "    2: 2870,\n",
    "    4: 3918,\n",
    "    8: 6012,\n",
    "    16: 10206,\n",
    "    32: 18595,\n",
    "}\n",
    "\n",
    "# M_activations is more complex\n",
    "print(f\"{'batch size':10s} {'M_activations':13s} {'total memory':12s} {'  diff':6s}\")\n",
    "for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "    M_activations = compute_activations(model_cfg, batch_size, dtype)\n",
    "    total_memory = M_model + M_optimizer + M_activations[\"total\"] + M_gradient\n",
    "    total_memory_GiB = total_memory / divisor\n",
    "    diff = (empirical_peak_memory_usage[batch_size] / 1024) - total_memory_GiB\n",
    "    print(\n",
    "        f\"{batch_size:10d} {M_activations['total'] / divisor:13.2f} {total_memory_GiB:12.2f} {diff:6.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOPS\n",
    "\n",
    "Here we estimate FLOPS for a single forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 flops          ratio (%) \n",
      "attention/wq             8589934592     0.3396\n",
      "attention/wk             4294967296     0.1698\n",
      "attention/wv             4294967296     0.1698\n",
      "attention/wo             8589934592     0.3396\n",
      "attention/scores         4294967296     0.1698\n",
      "attention/reduce         4294967296     0.1698\n",
      "attention               34359738368     1.3582\n",
      "mlp/w1                  30064771072     1.1885\n",
      "mlp/w2                  30064771072     1.1885\n",
      "mlp/w3                  30064771072     1.1885\n",
      "mlp                     90194313216     3.5654\n",
      "block                  124554051584     4.9236\n",
      "transformer           1992864825344    78.7776\n",
      "out_embedding          536870912000    21.2224\n",
      "forward_total         2529735737344   100.0000\n",
      "backward_total        5059471474688   200.0000\n",
      "total                 7589207212032   300.0000\n"
     ]
    }
   ],
   "source": [
    "def compute_flops(cfg: Llama3Config):\n",
    "    # we only count Weight FLOPs,\n",
    "    # FLOPS for all other layers (LayerNorm, Softmax, etc) and bias vector additian are effectively irrelevant\n",
    "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
    "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    d_head = cfg.d_model // cfg.n_head\n",
    "\n",
    "    # attention blocks\n",
    "    # 1) the projection to key, query, values\n",
    "    out[\"attention/wq\"] = 2 * cfg.n_ctx * cfg.d_model * cfg.d_model\n",
    "    out[\"attention/wk\"] = 2 * cfg.n_ctx * cfg.d_model * cfg.n_kv_head * d_head\n",
    "    out[\"attention/wv\"] = 2 * cfg.n_ctx * cfg.d_model * cfg.n_kv_head * d_head\n",
    "    out[\"attention/wo\"] = 2 * cfg.n_ctx * cfg.d_model * cfg.d_model\n",
    "    # 2) calculating the attention scores\n",
    "    out[\"attention/scores\"] = 2 * cfg.n_ctx * cfg.n_ctx * cfg.d_model\n",
    "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out[\"attention/reduce\"] = 2 * cfg.n_head * (cfg.n_ctx * cfg.n_ctx * d_head)\n",
    "    out[\"attention\"] = (\n",
    "        out[\"attention/wq\"]\n",
    "        + out[\"attention/wk\"]\n",
    "        + out[\"attention/wv\"]\n",
    "        + out[\"attention/wo\"]\n",
    "        + out[\"attention/scores\"]\n",
    "        + out[\"attention/reduce\"]\n",
    "    )\n",
    "    # MLP blocks\n",
    "    out[\"mlp/w1\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.intermediate_size)\n",
    "    out[\"mlp/w2\"] = 2 * cfg.n_ctx * (cfg.intermediate_size * cfg.d_model)\n",
    "    out[\"mlp/w3\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.intermediate_size)\n",
    "    out[\"mlp\"] = out[\"mlp/w1\"] + out[\"mlp/w2\"] + out[\"mlp/w3\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = cfg.n_layer * out[\"block\"]\n",
    "    out[\"out_embedding\"] = 2 * cfg.n_ctx * (cfg.d_model * cfg.vocab_size)\n",
    "\n",
    "    # forward,backward,total\n",
    "    out[\"forward_total\"] = out[\"transformer\"] + out[\"out_embedding\"]\n",
    "    out[\"backward_total\"] = (\n",
    "        2 * out[\"forward_total\"]\n",
    "    )  # use common estimate of bwd = 2*fwd\n",
    "    out[\"total\"] = out[\"forward_total\"] + out[\"backward_total\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# compare our param count to that reported by PyTorch\n",
    "model_flops = compute_flops(model_cfg)\n",
    "flops_total = model_flops[\"forward_total\"]\n",
    "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
    "for k, v in model_flops.items():\n",
    "    print(f\"{k:20s} {v:14d} {v / flops_total * 100:10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palm_flops: 7589622448128, flops: 7589207212032, ratio: 1.0001\n"
     ]
    }
   ],
   "source": [
    "# now here is an estimate copy pasted from the PaLM paper\n",
    "# this formula is often used to calculate MFU (model flops utilization)\n",
    "def compute_palm_flops(cfg: Llama3Config):\n",
    "    \"\"\"estimate of the model flops following PaLM paper formula\"\"\"\n",
    "    # non-embedding model parameters.\n",
    "    model_params = get_params(cfg)\n",
    "    N = model_params[\"total\"] - model_params[\"embedding\"]\n",
    "    L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.d_model // cfg.n_head, cfg.n_ctx\n",
    "    mf_per_token = 6 * N + 12 * L * H * Q * T\n",
    "    mf = mf_per_token * cfg.n_ctx\n",
    "    return mf\n",
    "\n",
    "\n",
    "palm_flops = compute_palm_flops(model_cfg)\n",
    "print(\n",
    "    f\"palm_flops: {palm_flops:d}, flops: {model_flops['total']:d}, ratio: {palm_flops / model_flops['total']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Flops Usage\n",
    "\n",
    "Given our estimated FLOPS we can calculate how much of our GPU FLOP capacity is being used, that is our model flop utilization (MFU).\n",
    "\n",
    "To calculate this we need a few bits of information:\n",
    "\n",
    "- GPU speed (FLOPS)\n",
    "- batch size (including gradient accumulation)\n",
    "- time per iteration\n",
    "\n",
    "For the GPU speed we refer to: https://www.techpowerup.com/gpu-specs/ which gives theoretical performance, specifically looking at performance for BF16 and FP16, which ever is supported and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of GPU FLOPS used\n",
      "GPU            ratio (%) \n",
      "H100               132.96\n",
      "A100               322.18\n",
      "RTX4090           1211.08\n",
      "RTX2070           6701.29\n"
     ]
    }
   ],
   "source": [
    "# R\n",
    "GPU_FLOPS = {\n",
    "    \"H100\": 756e12,  # 756 TFLOPS BF16 (this is a guess, spec sheet shows 1513 for sparse tensors)\n",
    "    \"A100\": 312e12,  # 312 TFLOPS BF16\n",
    "    \"RTX4090\": 83e12,  # 83 TFLOPS FP16\n",
    "    \"RTX2070\": 15e12,  # 15 TFLOPS FP16\n",
    "}\n",
    "\n",
    "# TODO Change these values to desired values\n",
    "batch_size = 20\n",
    "grad_accum = 5\n",
    "measured_time = 0.755  # in seconds per iteration\n",
    "\n",
    "# calculate flops achieved\n",
    "total_batch_size = batch_size * grad_accum\n",
    "measured_throughput = total_batch_size / measured_time\n",
    "flops_achieved = model_flops[\"total\"] * measured_throughput\n",
    "\n",
    "# the fraction of the A100 that we are using:\n",
    "print(\"Fraction of GPU FLOPS used\")\n",
    "print(f\"{'GPU':14s} {'ratio (%)':10s}\")\n",
    "for k, v in GPU_FLOPS.items():\n",
    "    print(f\"{k:14s} {flops_achieved / v * 100:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Training Compute\n",
    "\n",
    "Here we use the value computed so far to compute an estimate of the total amount of compute needed to train the model.\n",
    "\n",
    "Here we estimate the total amount of compute `C` required to train the model as `C ~= 6*N*D`, where:\n",
    "\n",
    "- `6` is a heuristic value (see [Dzmitry Bahdanau's post](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) for an explanation) \n",
    "    - it basically stems from weight multiplications requiring 6 FLOPS per token per weight when combining both forward and backward passes.\n",
    "- `N` is the total number of model parameters\n",
    "- `D` is total size of dataset (in tokens)\n",
    "\n",
    "Note the equation changes to `C ~= 8*N*D` with activation checkpointing (i.e. recompute activations as needed to save memory when doing back-prop).\n",
    "\n",
    "We also need to factor in the model flops utilization (MFU) to correct for the fact that we cannot use 100% of the GPUs FLOPS due to memory bottlenecks, etc (again see Dzmitry's blog for examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to train model on different GPUS\n",
      "GPU        time (days)     \n",
      "H100            16.42\n",
      "A100            39.79\n",
      "RTX4090        149.59\n",
      "RTX2070        827.73\n"
     ]
    }
   ],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "model_size = get_params(model_cfg)[\"total\"]  # this is number of parameters, N\n",
    "\n",
    "# TODO change these parameters\n",
    "tokens_num = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "num_gpus = 8  # number of GPUS used in parallel\n",
    "\n",
    "print(\"Time needed to train model on different GPUS\")\n",
    "print(f\"{'GPU':10s} {'time (days)':16s}\")\n",
    "for gpu_name, gpu_flops in GPU_FLOPS.items():\n",
    "    flops_throughput = gpu_flops * num_gpus * assumed_mfu\n",
    "    flops_needed = 6 * model_size * tokens_num  # 6ND\n",
    "    time_needed_s = flops_needed / flops_throughput  # in seconds\n",
    "    print(f\"{gpu_name:10s} {time_needed_s / 3600 / 24:10.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
