{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading experiments\n",
    "\n",
    "This notebook is for experimenting with different dataloading implementations and strategies.\n",
    "\n",
    "Some requirements:\n",
    "\n",
    "- working with very large datasets of text (100s of GBs) split across multiple files\n",
    "- need to be able to load data in distributed training across multiple GPUs (i.e. each GPU sees non-overlapping portion of the data)\n",
    "- need to be able to load batches of data (i.e. [batch_size, seq_len])\n",
    "- need to be able to load data in a streaming fashion, so that the whole dataset doesn't need to fit into memory at once\n",
    "- needs to include tokenization\n",
    "\n",
    "Desired properties:\n",
    "\n",
    "- fast\n",
    "- memory efficient\n",
    "- minimizes time spent loading data, so that the GPU is never waiting for the data to load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Dataset with Pytorch IterableDataset and DataLoader\n",
    "\n",
    "Since we alread use huggingface to get our datasets, let's try using huggingface's Dataset in combination with Pytorch's DataLoader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gollem.data.common import DATA_CACHE_DIR\n",
    "from gollem.tokenizer import get_tokenizer\n",
    "\n",
    "tinystories_dataset_id = \"roneneldan/Tinystories\"\n",
    "tinystories_ds_path = DATA_CACHE_DIR / \"tinystories\" / \"TinyStories_all_data\"\n",
    "\n",
    "tokenizer = get_tokenizer(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de9abbb4066444096f0ad4b4abe1e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description \n",
      "citation \n",
      "homepage \n",
      "license \n",
      "features {'story': Value(dtype='string', id=None), 'instruction': {'features': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'prompt:': Value(dtype='string', id=None), 'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'summary': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None)}\n",
      "post_processed None\n",
      "supervised_keys None\n",
      "builder_name json\n",
      "dataset_name tiny_stories_all_data\n",
      "config_name default\n",
      "version 0.0.0\n",
      "splits {'train': SplitInfo(name='train', num_bytes=6519810584, num_examples=4967871, shard_lengths=[400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 167871], dataset_name='tiny_stories_all_data')}\n",
      "download_checksums {'/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data00.json': {'num_bytes': 140424235, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data01.json': {'num_bytes': 140372109, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data02.json': {'num_bytes': 140537677, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data03.json': {'num_bytes': 140385817, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data04.json': {'num_bytes': 140540961, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data05.json': {'num_bytes': 140488084, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data06.json': {'num_bytes': 140408938, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data07.json': {'num_bytes': 140404430, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data08.json': {'num_bytes': 140427706, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data09.json': {'num_bytes': 140362835, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data10.json': {'num_bytes': 140285669, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data11.json': {'num_bytes': 140272677, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data12.json': {'num_bytes': 140358544, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data13.json': {'num_bytes': 140443405, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data14.json': {'num_bytes': 140131319, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data15.json': {'num_bytes': 140207809, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data16.json': {'num_bytes': 140232125, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data17.json': {'num_bytes': 140391792, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data18.json': {'num_bytes': 140473361, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data19.json': {'num_bytes': 140457799, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data20.json': {'num_bytes': 140369466, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data21.json': {'num_bytes': 140361164, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data22.json': {'num_bytes': 140482260, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data23.json': {'num_bytes': 140261146, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data24.json': {'num_bytes': 140318712, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data25.json': {'num_bytes': 140307496, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data26.json': {'num_bytes': 140527914, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data27.json': {'num_bytes': 140218254, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data28.json': {'num_bytes': 140632203, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data29.json': {'num_bytes': 140579114, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data30.json': {'num_bytes': 140508953, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data31.json': {'num_bytes': 140387626, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data32.json': {'num_bytes': 140303087, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data33.json': {'num_bytes': 140258740, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data34.json': {'num_bytes': 140346123, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data35.json': {'num_bytes': 140433120, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data36.json': {'num_bytes': 140221415, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data37.json': {'num_bytes': 140284995, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data38.json': {'num_bytes': 140396597, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data39.json': {'num_bytes': 140328168, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data40.json': {'num_bytes': 140440381, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data41.json': {'num_bytes': 140292864, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data42.json': {'num_bytes': 140389279, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data43.json': {'num_bytes': 140357607, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data44.json': {'num_bytes': 140259023, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data45.json': {'num_bytes': 140417405, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data46.json': {'num_bytes': 140369034, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data47.json': {'num_bytes': 140413474, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data48.json': {'num_bytes': 140324357, 'checksum': None}, '/Users/jonathon/code/gollem/gollem/data/datasets/tinystories/TinyStories_all_data/data49.json': {'num_bytes': 95236458, 'checksum': None}}\n",
      "download_size 6973633727\n",
      "post_processing_size None\n",
      "dataset_size 6519810584\n",
      "size_in_bytes 13493444311\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "from datasets import IterableDataset\n",
    "\n",
    "ds_builder = load_dataset_builder(path=str(tinystories_ds_path))\n",
    "for k, v in ds_builder.info.__dict__.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dbf386b8de4f5e80ed2af5d9701963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train']\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "\n",
    "print(get_dataset_split_names(path=str(tinystories_ds_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7222251f3e9d4f118817d45524022e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "ds = load_dataset(path=str(tinystories_ds_path), split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['story', 'instruction', 'summary', 'source'],\n",
       "    num_rows: 4967871\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4967871"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805eed856c624d20929da7990483a550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['story', 'instruction', 'summary', 'source']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import cast\n",
    "\n",
    "\n",
    "iter_ds = load_dataset(path=str(tinystories_ds_path), split=\"train\", streaming=True)\n",
    "iter_ds = cast(IterableDataset, iter_ds)\n",
    "iter_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: Unknown,\n",
      "    num_shards: 50\n",
      "})\n",
      "0 1 308 308\n",
      "1 1 200 200\n",
      "2 1 130 130\n",
      "3 1 159 159\n",
      "4 1 167 167\n",
      "5 1 199 199\n",
      "6 1 131 131\n",
      "7 1 170 170\n",
      "8 1 195 195\n",
      "9 1 192 192\n",
      "10 1 209 209\n",
      "11 1 167 167\n",
      "12 1 135 135\n",
      "13 1 189 189\n",
      "14 1 216 216\n",
      "15 1 212 212\n",
      "16 1 160 160\n",
      "17 1 167 167\n",
      "18 1 135 135\n",
      "19 1 162 162\n",
      "20 1 136 136\n",
      "21 1 138 138\n",
      "22 1 113 113\n",
      "23 1 139 139\n",
      "24 1 184 184\n",
      "25 1 414 414\n",
      "26 1 147 147\n",
      "27 1 208 208\n",
      "28 1 138 138\n",
      "29 1 191 191\n",
      "30 1 152 152\n",
      "31 1 199 199\n",
      "32 1 197 197\n",
      "33 1 143 143\n",
      "{'text': [[7454, 2402, 257, 640, 11, 612, 373, 257, 1310, 2576, 3706, 20037, 13, 1375, 2227, 257, 2495, 6576, 284, 5806, 284, 607, 1545, 338, 10955, 2151, 13, 2332, 1995, 1820, 1718, 607, 284, 262, 3650, 284, 1064, 530, 13, 220, 198, 43, 813, 2497, 257, 10283, 286, 20239, 27309, 10938, 319, 262, 3355, 13, 1375, 6235, 284, 257, 11398, 530, 290, 531, 11, 366, 40, 765, 326, 530, 2474, 220, 198, 29252, 1820, 13541, 290, 4193, 20037, 1949, 319, 262, 2495, 6576, 13, 632, 4197, 7138, 290, 20037, 2936, 588, 257, 21752, 13, 1375, 665, 49376, 1088, 290, 1088, 11, 3772, 351, 607, 649, 6576, 13, 220, 198, 2953, 262, 2151, 11, 20037, 338, 2460, 531, 673, 3114, 523, 2495, 287, 607, 11398, 6576, 13, 20037, 373, 845, 3772, 290, 2936, 2041, 13, 1375, 2993, 673, 550, 1043, 262, 2818, 6576, 13, 50256]]}\n"
     ]
    }
   ],
   "source": [
    "from typing import Generator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "iter_ds = cast(IterableDataset, iter_ds)\n",
    "# Strategy\n",
    "# 1. Use hugginface IterableDataset to load batches of examples\n",
    "# 2. tokenize each batch of examples on the fly\n",
    "# 3. concatenate tokens to get batch of seq_len tokens with no padding (collecting next batch of examples as necessary)\n",
    "# 4. repeat\n",
    "\n",
    "# Questions:\n",
    "# - how to make this work with distributed training?\n",
    "#    - can we customize the number of shards even if they are different from number of files?\n",
    "tokenizer = get_tokenizer(\"gpt2\")\n",
    "\n",
    "data_column = \"story\"\n",
    "# columns_to_remove = [c for c in iter_ds.column_names if c != data_column]\n",
    "\n",
    "\n",
    "def tokenize_batch(examples):\n",
    "    return {\"text\": tokenizer.encode_batch(examples[data_column])}\n",
    "\n",
    "\n",
    "# get batch from iter_ds\n",
    "processed_ds = iter_ds.map(\n",
    "    tokenize_batch, batched=True, batch_size=32, remove_columns=iter_ds.column_names\n",
    ")\n",
    "processed_ds = cast(IterableDataset, processed_ds)\n",
    "print(processed_ds)\n",
    "\n",
    "# get batch from batched_ds\n",
    "batched_ds = processed_ds.batch(batch_size=1)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 1024\n",
    "\n",
    "\n",
    "def get_batch() -> Generator[np.ndarray, None, None]:\n",
    "    buffer = [[] * batch_size]\n",
    "    current_batch = np.full((batch_size, seq_len), 0, dtype=np.long)\n",
    "    current_idxs = np.zeros(batch_size, dtype=np.int32)\n",
    "\n",
    "    for batch in batched_ds:\n",
    "        batch_lens = np.array([len(t) for t in batch[\"text\"]])\n",
    "        batch_idxs = seq_len - batch_lens\n",
    "        end_idxs = current_idxs\n",
    "\n",
    "        # current_batch[current_idxs:end_idxs] =\n",
    "\n",
    "\n",
    "for i, batch in enumerate(batched_ds):\n",
    "    print(\n",
    "        i,\n",
    "        len(batch[\"text\"]),\n",
    "        min(len(t) for t in batch[\"text\"]),\n",
    "        max(len(t) for t in batch[\"text\"]),\n",
    "    )\n",
    "    if i == 33:\n",
    "        break\n",
    "\n",
    "# get batch from batched_ds\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
